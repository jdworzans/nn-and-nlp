{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d711fdca",
   "metadata": {},
   "source": [
    "# Task 3 (4 points)\n",
    "\n",
    "Suppose that we have two languages: Upper and Lower. This is an example Upper sentence:\n",
    "\n",
    "<pre>\n",
    "THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG.\n",
    "</pre>\n",
    "\n",
    "And this is its translation into Lower:\n",
    "\n",
    "<pre>\n",
    "the quick brown fox jumps over the lazy dog\n",
    "</pre>\n",
    "\n",
    "You have two corpora for these languages (with different sentences). Your task is to train word embedings for both languages together, so as to make embeddings of the words which are its translations as close as possible. But unfortunately, you have the budget which allows you to prepare the translation only for 1000 words (we call it D, you have to deside which words you want to be in D)\n",
    "\n",
    "Prepare the corpora wich contains three kind of sentences:\n",
    "* Upper corpus sentences\n",
    "* Lower corpus sentences\n",
    "* sentences derived from Upper/Lower corpus, modified using D\n",
    "\n",
    "There are many possible ways of doing this, for instance this one (ROT13.COM: hfr rirel fragrapr sebz obgu pbecben gjvpr: jvgubhg nal zbqvsvpngvbaf, naq jvgu rirel jbeqf sebz Q ercynprq ol vgf genafyngvba)\n",
    "\n",
    "We define the score for an Upper WORD as  $\\frac{1}{p}$, where $p$ is a position of its translation in the list of **Lower** words most similar to WORD. For instance, when most similar words to DOG are:\n",
    "\n",
    "<pre>\n",
    "WOLF, CAT, WOLVES, LION, gopher, dog\n",
    "</pre>\n",
    "\n",
    "then the score for the word DOG is 0.5. Compute the average score separately for words from D, and for words out of D (hint: if the computation takes to much time do it for a random sample).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692da9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3709f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter()\n",
    "with open(\"../data/L4/task3_polish_lower.txt\", \"rt\") as f:\n",
    "    for words in map(str.split, f.readlines()):\n",
    "        c.update(filter(str.islower, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0daa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = [w for w, _ in c.most_common(1000)]\n",
    "nonD = [w for w, _ in c.most_common()][1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c05c9a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = {l: l.upper() for l in D}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "524c5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/L4/task3_polish_lower.txt\", \"rt\") as f_in, open(\"../data/L4/task3_polish_translations.txt\", \"wt\") as f_out:\n",
    "    for l in f_in:\n",
    "        f_out.write(\" \".join([translations.get(word, word) for word in l.split()]))\n",
    "        f_out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc0de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../data/L4/task3_polish* >../data/L4/task3_sentences.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    positions = {}\n",
    "    for d_word in tqdm(D):\n",
    "        position = 0\n",
    "        for word_idx in np.argsort(model.wv.most_similar(d_word, topn=None))[::-1]:\n",
    "            word = model.wv.index_to_key[word_idx]\n",
    "            if word.isupper():\n",
    "                position += 1\n",
    "                if word.lower() == d_word:\n",
    "                    positions[d_word] = position\n",
    "                    break\n",
    "    positions_D = np.array(list(positions.values()))\n",
    "    print(\"Average (D):\", (1 / positions_D).mean())\n",
    "\n",
    "    positions = {}\n",
    "    for nond_word in tqdm(nonD):\n",
    "        position = 0\n",
    "        for word_idx in np.argsort(model.wv.most_similar(nond_word, topn=None))[::-1]:\n",
    "            word = model.wv.index_to_key[word_idx]\n",
    "            if word.isupper():\n",
    "                position += 1\n",
    "                if word.lower() == nond_word:\n",
    "                    positions[nond_word] = position\n",
    "                    break\n",
    "    positions_nonD = np.array(list(positions.values()))\n",
    "    print(\"Average (outside D):\", (1 / positions_nonD).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 30\n",
      "Loaded pretrained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 129.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average (D): 0.6344844364838972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27286/27286 [03:31<00:00, 128.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average (outside D): 0.18288539900797587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for vector_size in [30]:\n",
    "    print(\"Vector size:\", vector_size)\n",
    "    filepath = Path(f\"../data/L4/task3_{vector_size}_model.model\")\n",
    "    if filepath.exists():\n",
    "        model = gensim.models.Word2Vec.load(str(filepath))\n",
    "        print(\"Loaded pretrained\")\n",
    "    else:\n",
    "        print(\"Training...\")\n",
    "        model = gensim.models.Word2Vec(corpus_file=\"../data/L4/task3_sentences.txt\", vector_size=vector_size, window=5, min_count=1, workers=6)\n",
    "        model.save(str(filepath))\n",
    "        print(\"Completed.\")\n",
    "    evaluate_model(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c9edea7d3c7f8afb1e3a5085762e5250f6cb461b15232d4b256ad3000efebc5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nn-and-nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
