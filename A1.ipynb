{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "96afc4de-06c2-44b0-9e0d-ec1df865f0dd",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "text-cell-h1",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "# NN 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-caefeb56-f9c5-4f9e-a096-b89246dc63d7",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 6
    },
    "deepnote_cell_type": "markdown",
    "id": "CGXgWugfJ0Vl"
   },
   "source": [
    "## Assignment 1\n",
    "\n",
    "**Submission deadlines:** \n",
    "- get at least **2** points by Tuesday, 15.03.2022\n",
    "- remaining points: last lab session before or on Tuesday, 22.03.2022\n",
    "\n",
    "**Points:** Aim to get 8 out of 12 possible points\n",
    "\n",
    "## Submission instructions\n",
    "The class is held on-site in lab rooms. Please prepare you notebook on your computer or anywhere in the cloud (try using DeepNote or Google Colab).\n",
    "Make sure you know all the questions and asnwers, and that the notebook contains results; bfore presentation do `Runtime -> Restart and run all`\n",
    "![Picture title](image-20220302-183151.png)\n",
    "\n",
    "We provide starter code, however you are not required to use it as long as you properly solve the tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-c92cc86e-f65f-4e16-9124-9ca4d4265c88",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 12
    },
    "deepnote_cell_type": "markdown",
    "id": "5S8iRaCPyO2a"
   },
   "source": [
    "# Task description\n",
    "\n",
    "## TLDR\n",
    "Implement and train a neural network using pure numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-1893a0a0-4515-4960-bce8-37f660543eea",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 18
    },
    "deepnote_cell_type": "markdown",
    "id": "JHcz7I2V-bVM"
   },
   "source": [
    "\n",
    "## Problem 1 [2p]\n",
    "Implement a two-layer network, manually set weights and biases to solve the XOR task.\n",
    "\n",
    "A two-layer neural network implementes a function $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}^O$ where $D$ is the input dimensionality and $O$ is the output dinemsionality. The output goes through an intermediate representation (the hidden layer) with dimensionality $H$. \n",
    "\n",
    "The computations are as follows:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A_1 &= x W_1^T + b_1  & \\qquad\\text{Total input to neurons in the hidden layer (network's first layer)} \\\\\n",
    "O_1 &= \\sigma_1(A_1)  & \\qquad\\text{Output of the hidden layer} \\\\\n",
    "A_2 &= O_1 W_2^T + b_2 & \\qquad\\text{Total input to neurons in the output layer (network's second layer)}\\\\\n",
    "O_2 &= \\sigma_2(A_2)  & \\qquad\\text{Output of the network}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $W$ are weight matrices, $b$ are bias vectors, $\\sigma$ are non-linear activation functions (e.g. the logistic sigmoid applied element-wise, or softmax).\n",
    "\n",
    "For the 2D xor problem the network will:\n",
    "- have 2 inputs, 2 hidden neurons, one output\n",
    "- use the logistic sigmoid everywhere (that way we, when hand-designig the weights, we can assume that neurons' outputs are binary).\n",
    "\n",
    "Therrefore the shapes of the data flowing through the network will be:\n",
    "- input: $x\\in\\mathbb{}R^{2}$\n",
    "- hidden layer parameters: $W_1\\in\\mathbb{}R^{2\\times 2}$ and $b_1\\in\\mathbb{}R^{2}$\n",
    "- representations in the hidden layer: $A_1\\in\\mathbb{}R^{2}$ and $O_1\\in\\mathbb{}R^{2}$\n",
    "- output layer parameters: $W_2\\in\\mathbb{}R^{1\\times 2}$ and $b_2\\in\\mathbb{}R^{1}$\n",
    "- representations in the output layer: $A_2\\in\\mathbb{}R^{1}$ and $O_2\\in\\mathbb{}R^{1}$\n",
    "\n",
    "The network can be seen as a logistic regression model, prefixed by a nonlinear transformation of the data.\n",
    "\n",
    "The first tasks consists of:\n",
    "- implementing the network\n",
    "- selecting parametwrs ($W_1, b_1, W_2, b_2$) such that $f(x)\\approx XOR(x_1, x_2)$ where the approximation is die to the sigmoids - the output may be close to 0 or 1, but doesn't need to saturate at 0 or 1.\n",
    "\n",
    "NB: the convention on weight matrix shapes follows linear [layers in PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-0d502510-cfd1-44da-b88c-da6aaf275617",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 24
    },
    "deepnote_cell_type": "markdown",
    "id": "0QSpZxuH-bLe"
   },
   "source": [
    "## Problem 2 [2p]\n",
    "1. Add a backward pass.\n",
    "2. Use a sensible random initialization for weights and biases.\n",
    "3. Numerically check the correctness of your gradient computation.\n",
    "\n",
    "There is nice article about taking derivative over vectors and vector chain rule: https://explained.ai/matrix-calculus/ if someone don't have experience with suchr calculus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-96083315-f850-4103-946e-cc9b11377543",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 30
    },
    "deepnote_cell_type": "markdown",
    "id": "s1Tn8j0m-bAy"
   },
   "source": [
    "## Problem 3 [2p]\n",
    "1. Implement gradient descent\n",
    "2. Train your network to solve 3D XOR\n",
    "3. Try several hidden layer sizes, for each size record the fracton of successful trainings. Then answer:\n",
    "    - What is the minimal hidden size required to solve 3D XOR (even with low reliability, when the training has to be repeated multiple times)\n",
    "    - What is the minimal hidden size required to reliably solve 3D XOR\n",
    "    - Which networks are easier to train - small or large ones? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-94cc4eb9-0202-4308-9a9e-007db44a6a98",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 36
    },
    "deepnote_cell_type": "markdown",
    "id": "RP9Pvpmf-a2A"
   },
   "source": [
    "## Problem 4 [1p]\n",
    "Replace the first nonlinearity with the [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation function. Find a network architecture which reliably learns the 3D XOR problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-9e6e984c-5131-4333-837b-7e57c4bfb650",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 42
    },
    "deepnote_cell_type": "markdown",
    "id": "cGgtpe-w-asB"
   },
   "source": [
    "## Problem 5 [1p]\n",
    "Add a second hidden layer to your network, implement the forward and backward pass, then demonstrate training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-86ea4bdd-83dc-4197-baad-d216fbacc786",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 48
    },
    "deepnote_cell_type": "markdown",
    "id": "Pe-pcFeO-aiE"
   },
   "source": [
    "## Problem 6 [2p]\n",
    "Implement a way to have a _variable number_ of hidden layers. Check how deep sigmoid or ReLU networks you  can train. For simplicity you can assume that all hidden layers have the same number of neurons, and use the same activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-64a0b978-7933-40be-89e4-a3d35d52c7ab",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 54
    },
    "deepnote_cell_type": "markdown",
    "id": "kIpn17Cm-aW7"
   },
   "source": [
    "## Problem 7 [2p]\n",
    "For each weight matrix $w\\in\\mathbb{R}^{n\\times m}$, add a randomly initialized `backward weight` $w_b\\in\\mathbb{R}^{m\\times n}$, which will not change during training. Change the backward pass to use $w_b$ instead of $w^T$, getting an approxmatoin of the true gradient. Can you get your network to train?\n",
    "\n",
    "NB: this approach, dubbed [feedback alignment](https://www.nature.com/articles/ncomms13276), was proposed to make error backpropagation more biologically plausible, by providing a solution to the \"weight transport problem\". Regular backpropagation requires that neurons not only know their incoming weights (thet they control), but also their outgoing weights (that are controlled by neurons in the upper layers). This is nearly impossible in a real brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-954fd65b-4d81-4bbc-bf6b-4d10d50be9b8",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 60
    },
    "deepnote_cell_type": "markdown",
    "id": "rXJaoHSH0DZO"
   },
   "source": [
    "# Solutions and starter code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00011-97f2c7fe-a3d5-4a6b-bc12-8a96bfeed475",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 66
    },
    "deepnote_cell_type": "code",
    "id": "YiTEWD2oqW0Y"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-8bc44e88-e665-483d-89fc-c866879ad335",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 72
    },
    "deepnote_cell_type": "markdown",
    "id": "eqtfJKR40J3x"
   },
   "source": [
    "XOR dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00013-3adfc55c-5dcc-476a-9e4a-328f2044ebc1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 78
    },
    "deepnote_cell_type": "code",
    "id": "lYEbCfbSpv5M",
    "outputId": "48a99aad-e15b-4c7b-f881-bbfbe1941a15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.05, 1.05, -0.05, 1.05)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's define a XOR dataset\n",
    "\n",
    "# X will be matrix of N 2-dimensional inputs\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1],], dtype=np.float32)\n",
    "# Y is a matrix of N numners - answers\n",
    "Y = np.array([[0], [1], [1], [0],], dtype=np.float32)\n",
    "\n",
    "plt.scatter(\n",
    "    X[:, 0], X[:, 1], c=Y[:, 0],\n",
    ")\n",
    "plt.xlabel(\"X[0]\")\n",
    "plt.ylabel(\"X[1]\")\n",
    "plt.axis(\"square\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-fe501b87-4247-497d-a7c7-f528483d31cc",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 84
    },
    "deepnote_cell_type": "markdown",
    "id": "Rb3azMn929_I"
   },
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-97a3483d-0ae6-4bee-afd5-6226abc622a5",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 90
    },
    "deepnote_cell_type": "markdown",
    "id": "RZCM_hdELE04"
   },
   "source": [
    "The code below contains a mock-up of a two-layer neural network. Fill in the code and manually set weights to solve the XOR problem.\n",
    "\n",
    "Please note: the shapes are set to be compatible with PyTorch's conventions:\n",
    "* a batch containing $N$ $D$-dimensional examples has shape $N\\times D$ (each example is a row!)\n",
    "* a weight matrix in a linear layer with $I$ inputs and $O$ outputs has shape $O \\times I$\n",
    "* a bias vector is a 1D vector. Please note that [broadcasting rules](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) allow us to think about it as a $1 \\times D$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = np.linspace(-5, 5)\n",
    "plt.plot(t, sigmoid(t))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "00016-87dd53c6-4f21-4286-8a85-61c2a92fa819",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 96
    },
    "deepnote_cell_type": "code",
    "id": "lrrRuk6zLiF0"
   },
   "outputs": [],
   "source": [
    "class SmallNet:\n",
    "    def __init__(self, in_features, num_hidden, dtype=np.float32):\n",
    "        self.W1 = np.zeros((num_hidden, in_features), dtype=dtype)\n",
    "        self.b1 = np.zeros((num_hidden,), dtype=dtype)\n",
    "        self.W2 = np.zeros((1, num_hidden), dtype=dtype)\n",
    "        self.b2 = np.zeros((1,), dtype=dtype)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        self.W1 = np.random.normal(0, 0.5, self.W1.shape)\n",
    "        self.b1 = np.random.normal(0, 0.5, self.b1.shape)\n",
    "        self.W2 = np.random.normal(0, 0.5, self.W2.shape)\n",
    "        self.b2 = np.random.normal(0, 0.5, self.b2.shape)\n",
    "\n",
    "    def forward(self, X, Y=None, do_backward=False):\n",
    "        # TODO Problem 1: Fill in details of forward propagation\n",
    "\n",
    "        # Input to neurons in 1st layer\n",
    "        A1 = np.dot(X, self.W1.T) + self.b1\n",
    "        # Outputs after the sigmoid non-linearity\n",
    "        O1 = sigmoid(A1)\n",
    "        # print(O1)\n",
    "        # Inputs to neuron in the second layer\n",
    "        A2 = np.dot(O1, self.W2.T) + self.b2\n",
    "        # Outputs after the sigmoid non-linearity\n",
    "        O2 = sigmoid(A2)\n",
    "\n",
    "        # When Y is none, simply return the predictions. Else compute the loss\n",
    "        if Y is not None:\n",
    "            loss = -np.sum((1 - Y) * np.log(1 - O2) + Y * np.log(O2)) # TODO cross-entropy loss\n",
    "            # normalize loss by batch size\n",
    "            loss = loss.sum() / X.shape[0]\n",
    "        else:\n",
    "            loss = np.nan\n",
    "\n",
    "        if do_backward:\n",
    "            # TODO in Problem 2:\n",
    "            # fill in the gradient computation/\n",
    "            # Please note, that there is a correspondance between\n",
    "            # the forward and backward pass: with backward computations happening\n",
    "            # in reverse order.\n",
    "            # We save the gradients with respect to the parameters as fields of self.\n",
    "            # It is not very elegant, but simplifies training code later on.\n",
    "\n",
    "            # A2_grad is the gradient of loss with respect to A2\n",
    "            # Hint: there is a concise formula for the gradient\n",
    "            # of logistic sigmoid and cross-entropy loss\n",
    "            A2_grad = (O2 - Y) / X.shape[0]\n",
    "            self.b2_grad = A2_grad.sum(0)\n",
    "            self.W2_grad = np.dot(A2_grad.T, O1)\n",
    "            O1_grad = np.dot(A2_grad, self.W2)\n",
    "            A1_grad = O1_grad * O1 * (1 - O1)\n",
    "            self.b1_grad = A1_grad.sum(0)\n",
    "            self.W1_grad = np.dot(A1_grad.T, X)\n",
    "\n",
    "        return O2, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00017-32ce9293-0a37-4397-aeac-25eef428e8d4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 102
    },
    "deepnote_cell_type": "code",
    "id": "jJswvBk0oiIY",
    "outputId": "e6559317-7afa-4509-fbac-4880e73b91cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XORnet([0. 0.]) = 0.007644135828260396\n",
      "XORnet([0. 1.]) = 0.9928472118111036\n",
      "XORnet([1. 0.]) = 0.9928472118111036\n",
      "XORnet([1. 1.]) = 0.007644135828260396\n"
     ]
    }
   ],
   "source": [
    "# TODO Problem 1:\n",
    "# Set by hand the weight values to solve the XOR problem\n",
    "\n",
    "net = SmallNet(2, 2, dtype=np.float64)\n",
    "net.W1 = 10 * np.array([[1, -1], [-1, 1]])\n",
    "net.b1 = -5 * np.array([1, 1])\n",
    "net.W2 = 10 * np.array([[1, 1]])\n",
    "net.b2 = 5 * np.array([-1])\n",
    "\n",
    "# Hint: since we use the logistic sigmoid activation, the weights may need to\n",
    "# be fairly large\n",
    "\n",
    "predictions, loss = net.forward(X, Y)#, do_backward=True)\n",
    "for x, p in zip(X, predictions):\n",
    "    print(f\"XORnet({x}) = {p[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-9f8740d2-90e7-4d20-a535-002df8547aba",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 108
    },
    "deepnote_cell_type": "markdown",
    "id": "wmxCi5Vl6_xB"
   },
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00019-8f804996-a00a-4bb5-a524-ad056659e023",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 114
    },
    "deepnote_cell_type": "code",
    "id": "eSM5hgJ1mrhY"
   },
   "outputs": [],
   "source": [
    "def check_grad(net, param_name, X, Y, eps=1e-5):\n",
    "    \"\"\"A gradient checking routine\"\"\"\n",
    "\n",
    "    param = getattr(net, param_name)\n",
    "    param_flat_accessor = param.reshape(-1)\n",
    "\n",
    "    grad = np.empty_like(param)\n",
    "    grad_flat_accessor = grad.reshape(-1)\n",
    "\n",
    "    net.forward(X, Y, do_backward=True)\n",
    "    orig_grad = getattr(net, param_name + \"_grad\")\n",
    "    assert param.shape == orig_grad.shape, f\"{param_name} size doesn't match the gradient shape\"\n",
    "\n",
    "    for i in range(param_flat_accessor.shape[0]):\n",
    "        orig_val = param_flat_accessor[i]\n",
    "        param_flat_accessor[i] = orig_val + eps\n",
    "        _, loss_positive = net.forward(X, Y)\n",
    "        param_flat_accessor[i] = orig_val - eps\n",
    "        _, loss_negative = net.forward(X, Y)\n",
    "        param_flat_accessor[i] = orig_val\n",
    "        grad_flat_accessor[i] = (loss_positive - loss_negative) / (2 * eps)\n",
    "    assert np.allclose(grad, orig_grad)\n",
    "    return grad, orig_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "00020-1c4174cf-f129-4cad-adad-4734810a8ce1",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 120
    },
    "deepnote_cell_type": "code",
    "id": "TTZu0jFEvgXF"
   },
   "outputs": [],
   "source": [
    "# Hint: use float64 for checking the correctness of the gradient\n",
    "net = SmallNet(2, 2, dtype=np.float64)\n",
    "\n",
    "for param_name in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
    "    check_grad(net, param_name, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00021-145837f8-c83f-4aab-8e62-0114e59f30fe",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 126
    },
    "deepnote_cell_type": "markdown",
    "id": "8mUOs3cVvjM2"
   },
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "00022-91b3eba0-6025-4839-847c-f5cb275fad07",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 132
    },
    "deepnote_cell_type": "code",
    "id": "nn2AAoZo0vjU",
    "outputId": "7b6b5a9a-dea5-4357-e8c1-36a34c7c272a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 steps \tloss=0.7963235472814381\n",
      "after 5000 steps \tloss=0.033322186521434274\n",
      "after 10000 steps \tloss=0.011005403680906517\n",
      "after 15000 steps \tloss=0.0062703534349069605\n",
      "after 20000 steps \tloss=0.0042910466885084835\n",
      "after 25000 steps \tloss=0.0032218959906317804\n",
      "after 30000 steps \tloss=0.002558949362098815\n",
      "after 35000 steps \tloss=0.0021106032189827416\n",
      "after 40000 steps \tloss=0.001788693872654549\n",
      "after 45000 steps \tloss=0.0015472111007791018\n",
      "after 50000 steps \tloss=0.0013598859658445366\n",
      "after 55000 steps \tloss=0.0012106720820751306\n",
      "after 60000 steps \tloss=0.0010892363848784012\n",
      "after 65000 steps \tloss=0.000988636677317215\n",
      "after 70000 steps \tloss=0.0009040434783055841\n",
      "after 75000 steps \tloss=0.0008319976122181747\n",
      "after 80000 steps \tloss=0.0007699592221620443\n",
      "after 85000 steps \tloss=0.000716023246513142\n",
      "after 90000 steps \tloss=0.0006687340029935135\n",
      "after 95000 steps \tloss=0.0006269609172573651\n"
     ]
    }
   ],
   "source": [
    "net = SmallNet(2, 10, dtype=np.float64)\n",
    "\n",
    "alpha = 0.1  # set a learning rate\n",
    "\n",
    "for i in range(100000):\n",
    "    _, loss = net.forward(X, Y, do_backward=True)\n",
    "    if (i % 5000) == 0:\n",
    "        print(f\"after {i} steps \\tloss={loss}\")\n",
    "    for param_name in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
    "        param = getattr(net, param_name)\n",
    "        # Hint: use the construct `param[:]` to change the contents of the array!\n",
    "        # Doing instead `param = new_val` simply changes to what the variable\n",
    "        # param points to, without affecting the network!\n",
    "        # alternatively, you could do setattr(net, param_name, new_value)\n",
    "        param[:] = param[:] - alpha * getattr(net, param_name + \"_grad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "00023-c92941a8-24d1-46e8-a410-d189c1d87b08",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 138
    },
    "deepnote_cell_type": "code",
    "id": "TwpEjpkU1JvK",
    "outputId": "dc044de9-81c1-4944-d9a2-5dcc72bf9a57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XORnet([0. 0.]) = 0.0003580865070048508\n",
      "XORnet([0. 1.]) = 0.9994236549835024\n",
      "XORnet([1. 0.]) = 0.9993779237235279\n",
      "XORnet([1. 1.]) = 0.000801999218353014\n"
     ]
    }
   ],
   "source": [
    "predictions, loss = net.forward(X, Y, do_backward=True)\n",
    "for x, p in zip(X, predictions):\n",
    "    print(f\"XORnet({x}) = {p[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat, product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 1, 0],\n",
       "        [0, 1, 1],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 1],\n",
       "        [1, 1, 0],\n",
       "        [1, 1, 1]]),\n",
       " array([[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3 = np.array(list(product(*repeat([0, 1], 3))))\n",
    "Y3 = np.logical_xor.reduce(X3, axis=1, keepdims=True).astype(float)\n",
    "X3, Y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(101) == np.log10(101).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(101).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "cell_id": "00024-e09655a3-84e1-4a43-9b61-00edb56c6054",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 144
    },
    "deepnote_cell_type": "code",
    "id": "U0ZMyHqz8xrC"
   },
   "outputs": [],
   "source": [
    "alpha = 0.1  # set a learning rate\n",
    "n_repeats = 10\n",
    "results = []\n",
    "\n",
    "for hidden_dim in [2, 3, 5, 10, 20]:\n",
    "    for _ in range(n_repeats):\n",
    "        net = SmallNet(3, hidden_dim, dtype=np.float64)\n",
    "        for i in range(1, 100001):\n",
    "            _, loss = net.forward(X3, Y3, do_backward=True)\n",
    "            for param_name in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
    "                param = getattr(net, param_name)\n",
    "                param[:] = param[:] - alpha * getattr(net, param_name + \"_grad\")\n",
    "            \n",
    "            if (np.log10(i) == np.log10(i).round()):\n",
    "                results.append(\n",
    "                    {\"epochs\": i, \"accuracy\": (net.forward(X3)[0].round() == Y3).mean(), \"loss\": loss, \"hidden_dim\": hidden_dim}\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x=\"epochs\", y=\"accuracy\", hue=\"hidden_dim\", data=pd.DataFrame(results).groupby([\"hidden_dim\", \"epochs\"]).mean()[\"accuracy\"].reset_index())\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x=\"epochs\", y=\"loss\", hue=\"hidden_dim\", data=pd.DataFrame(results).groupby([\"hidden_dim\", \"epochs\"]).mean()[\"loss\"].reset_index())\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00025-9703bd09-6aae-4830-8221-09b53a623e4b",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 150
    },
    "deepnote_cell_type": "markdown",
    "id": "UuaLEoV-9DLG"
   },
   "source": [
    "## Problem 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "cell_id": "00026-1a2be430-91e0-4209-ab3f-9ef84c39513b",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 156
    },
    "deepnote_cell_type": "code",
    "id": "w3lk9_TM-MvK"
   },
   "outputs": [],
   "source": [
    "class ReLUNet(SmallNet):\n",
    "    def forward(self, X, Y=None, do_backward=False):\n",
    "        # Input to neurons in 1st layer\n",
    "        A1 = np.dot(X, self.W1.T) + self.b1\n",
    "        # Outputs after the ReLU non-linearity\n",
    "        O1 = np.fmax(0, A1)\n",
    "        # Inputs to neuron in the second layer\n",
    "        A2 = np.dot(O1, self.W2.T) + self.b2\n",
    "        # Outputs after the sigmoid non-linearity\n",
    "        O2 = sigmoid(A2)\n",
    "\n",
    "        # When Y is none, simply return the predictions. Else compute the loss\n",
    "        if Y is not None:\n",
    "            loss = -np.sum((1 - Y) * np.log(1 - O2) + Y * np.log(O2)) # TODO cross-entropy loss\n",
    "            # normalize loss by batch size\n",
    "            loss = loss.sum() / X.shape[0]\n",
    "        else:\n",
    "            loss = np.nan\n",
    "\n",
    "        if do_backward:\n",
    "            # Please note, that there is a correspondance between\n",
    "            # the forward and backward pass: with backward computations happening\n",
    "            # in reverse order.\n",
    "            # We save the gradients with respect to the parameters as fields of self.\n",
    "            # It is not very elegant, but simplifies training code later on.\n",
    "\n",
    "            # A2_grad is the gradient of loss with respect to A2\n",
    "            # Hint: there is a concise formula for the gradient\n",
    "            # of logistic sigmoid and cross-entropy loss\n",
    "            A2_grad = (O2 - Y) / X.shape[0]\n",
    "            self.b2_grad = A2_grad.sum(0)\n",
    "            self.W2_grad = np.dot(A2_grad.T, O1)\n",
    "            O1_grad = np.dot(A2_grad, self.W2)\n",
    "            A1_grad = O1_grad * np.where(O1 == 0, 0, 1)\n",
    "            self.b1_grad = A1_grad.sum(0)\n",
    "            self.W1_grad = np.dot(A1_grad.T, X)\n",
    "\n",
    "        return O2, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 steps \tloss=0.7361145519393121\n",
      "after 5000 steps \tloss=0.5553701298489833\n",
      "after 10000 steps \tloss=0.38736705463269394\n",
      "after 15000 steps \tloss=0.1515391389085064\n",
      "after 20000 steps \tloss=0.06254608401948614\n",
      "after 25000 steps \tloss=0.03338646154022652\n",
      "after 30000 steps \tloss=0.021336251156697975\n",
      "after 35000 steps \tloss=0.015183260813445563\n",
      "after 40000 steps \tloss=0.011560053458949032\n",
      "after 45000 steps \tloss=0.009242154764854656\n",
      "after 50000 steps \tloss=0.007640211105429329\n",
      "after 55000 steps \tloss=0.0064709258736653005\n",
      "after 60000 steps \tloss=0.005586773521455856\n",
      "after 65000 steps \tloss=0.004896887485151577\n",
      "after 70000 steps \tloss=0.0043464060093732805\n",
      "after 75000 steps \tloss=0.0038979663445852107\n",
      "after 80000 steps \tloss=0.0035265935592791284\n",
      "after 85000 steps \tloss=0.003214436176279916\n",
      "after 90000 steps \tloss=0.0029489621119440515\n",
      "after 95000 steps \tloss=0.002720848055011697\n"
     ]
    }
   ],
   "source": [
    "net = ReLUNet(3, 10)\n",
    "\n",
    "alpha = 0.01  # set a learning rate\n",
    "\n",
    "for i in range(100000):\n",
    "    _, loss = net.forward(X3, Y3, do_backward=True)\n",
    "    if (i % 5000) == 0:\n",
    "        print(f\"after {i} steps \\tloss={loss}\")\n",
    "    for param_name in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
    "        param = getattr(net, param_name)\n",
    "        # Hint: use the construct `param[:]` to change the contents of the array!\n",
    "        # Doing instead `param = new_val` simply changes to what the variable\n",
    "        # param points to, without affecting the network!\n",
    "        # alternatively, you could do setattr(net, param_name, new_value)\n",
    "        param[:] = param[:] - alpha * getattr(net, param_name + \"_grad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy: {(net.forward(X3)[0].round() == Y3).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00027-de8a387d-3b6a-48d5-b6c8-fddf13637e38",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 162
    },
    "deepnote_cell_type": "markdown",
    "id": "_Hr_iAKX-ND1"
   },
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNet:\n",
    "    def __init__(self, in_features, num_hidden, dtype=np.float32):\n",
    "        self.W1 = np.zeros((num_hidden[0], in_features), dtype=dtype)\n",
    "        self.b1 = np.zeros((num_hidden[0],), dtype=dtype)\n",
    "        self.W2 = np.zeros((num_hidden[1], num_hidden[0]), dtype=dtype)\n",
    "        self.b2 = np.zeros((num_hidden[1],), dtype=dtype)\n",
    "        self.W3 = np.zeros((1, num_hidden[1]), dtype=dtype)\n",
    "        self.b3 = np.zeros((1,), dtype=dtype)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        self.W1 = np.random.normal(0, 0.5, self.W1.shape)\n",
    "        self.b1 = np.random.normal(0, 0.5, self.b1.shape)\n",
    "        self.W2 = np.random.normal(0, 0.5, self.W2.shape)\n",
    "        self.b2 = np.random.normal(0, 0.5, self.b2.shape)\n",
    "        self.W3 = np.random.normal(0, 0.5, self.W3.shape)\n",
    "        self.b3 = np.random.normal(0, 0.5, self.b3.shape)\n",
    "\n",
    "    def forward(self, X, Y=None, do_backward=False):\n",
    "        # Input to neurons in 1st layer\n",
    "        A1 = np.dot(X, self.W1.T) + self.b1\n",
    "        O1 = sigmoid(A1)\n",
    "        \n",
    "        A2 = np.dot(O1, self.W2.T) + self.b2\n",
    "        O2 = sigmoid(A2)\n",
    "\n",
    "        A3 = np.dot(O2, self.W3.T) + self.b3\n",
    "        O3 = sigmoid(A3)\n",
    "\n",
    "        # When Y is none, simply return the predictions. Else compute the loss\n",
    "        if Y is not None:\n",
    "            loss = -np.sum((1 - Y) * np.log(1 - O3) + Y * np.log(O3)) # TODO cross-entropy loss\n",
    "            # normalize loss by batch size\n",
    "            loss = loss.sum() / X.shape[0]\n",
    "        else:\n",
    "            loss = np.nan\n",
    "\n",
    "        if do_backward:\n",
    "            A3_grad = (O3 - Y) / X.shape[0]\n",
    "            self.b3_grad = A3_grad.sum(0)\n",
    "            self.W3_grad = np.dot(A3_grad.T, O2)\n",
    "\n",
    "            O2_grad = np.dot(A3_grad, self.W3)\n",
    "            A2_grad = O2_grad * O2 * (1 - O2)\n",
    "            self.b2_grad = A2_grad.sum(0)\n",
    "            self.W2_grad = np.dot(A2_grad.T, O1)\n",
    "\n",
    "            O1_grad = np.dot(A2_grad, self.W2)\n",
    "            A1_grad = O1_grad * O1 * (1 - O1)\n",
    "            self.b1_grad = A1_grad.sum(0)\n",
    "            self.W1_grad = np.dot(A1_grad.T, X)\n",
    "\n",
    "        return O3, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "cell_id": "00028-0aa6ea62-907d-44bf-9ba4-71870d3dd547",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 168
    },
    "deepnote_cell_type": "code",
    "id": "rnz6CndQ-NRI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 steps \tloss=0.8008456631278243\n",
      "after 5000 steps \tloss=0.6931352997848874\n",
      "after 10000 steps \tloss=0.693121623008466\n",
      "after 15000 steps \tloss=0.6930991263056884\n",
      "after 20000 steps \tloss=0.6930515217312132\n",
      "after 25000 steps \tloss=0.6929068774459468\n",
      "after 30000 steps \tloss=0.6918888643011369\n",
      "after 35000 steps \tloss=0.6081977770031318\n",
      "after 40000 steps \tloss=0.06641644967084001\n",
      "after 45000 steps \tloss=0.01426677280956595\n",
      "after 50000 steps \tloss=0.007138858892015345\n",
      "after 55000 steps \tloss=0.004600775090662692\n",
      "after 60000 steps \tloss=0.0033404723992621877\n",
      "after 65000 steps \tloss=0.0025985769292876286\n",
      "after 70000 steps \tloss=0.0021141274149681154\n",
      "after 75000 steps \tloss=0.0017748917525165046\n",
      "after 80000 steps \tloss=0.001525091102102796\n",
      "after 85000 steps \tloss=0.001334038404760391\n",
      "after 90000 steps \tloss=0.0011835262591507038\n",
      "after 95000 steps \tloss=0.0010621014726378516\n"
     ]
    }
   ],
   "source": [
    "net = DeepNet(3, [2, 2], dtype=np.float64)\n",
    "\n",
    "alpha = 0.1  # set a learning rate\n",
    "\n",
    "for i in range(100000):\n",
    "    _, loss = net.forward(X3, Y3, do_backward=True)\n",
    "    if (i % 5000) == 0:\n",
    "        print(f\"after {i} steps \\tloss={loss}\")\n",
    "    for param_name in [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"]:\n",
    "        param = getattr(net, param_name)\n",
    "        # Hint: use the construct `param[:]` to change the contents of the array!\n",
    "        # Doing instead `param = new_val` simply changes to what the variable\n",
    "        # param points to, without affecting the network!\n",
    "        # alternatively, you could do setattr(net, param_name, new_value)\n",
    "        param[:] = param[:] - alpha * getattr(net, param_name + \"_grad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy: {(net.forward(X3)[0].round() == Y3).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00029-1de44bc6-5f35-4df3-af9a-408291ccc867",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 174
    },
    "deepnote_cell_type": "markdown",
    "id": "4PcNxrCt-NcN"
   },
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "cell_id": "00030-fdaced74-34ab-4cfc-8fa2-f3a0b012fbf2",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 180
    },
    "deepnote_cell_type": "code",
    "id": "6Brepirl-Nln"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Net:\n",
    "    def __init__(self, in_features, num_hidden: List[int], dtype=np.float32):\n",
    "        # First layer\n",
    "        self.W = [np.zeros((num_hidden[0], in_features), dtype=dtype)]\n",
    "        self.b = [np.zeros((num_hidden[0],), dtype=dtype)]\n",
    "\n",
    "        # Add other layers\n",
    "        for i in range(1, len(num_hidden)):\n",
    "            self.W.append(np.zeros((num_hidden[i], num_hidden[i - 1]), dtype=dtype))\n",
    "            self.b.append(np.zeros((num_hidden[i], ), dtype=dtype))\n",
    "\n",
    "        # Last layer\n",
    "        self.W.append(np.zeros((1, num_hidden[-1]), dtype=dtype))\n",
    "        self.b.append(np.zeros((1, ), dtype=dtype))\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for W, b in zip(self.W, self.b):\n",
    "            W[:] = np.random.normal(0, 0.5, W.shape)\n",
    "            b[:] = np.random.normal(0, 0.5, b.shape)\n",
    "\n",
    "    def forward(self, X, Y=None, do_backward=False):\n",
    "        # Input to neurons in 1st layer\n",
    "        A = [np.dot(X, self.W[0].T) + self.b[0]]\n",
    "        O = [sigmoid(A[0])]\n",
    "\n",
    "        for W, b in zip(self.W[1:], self.b[1:]):\n",
    "            A.append(np.dot(O[-1], W.T) + b)\n",
    "            O.append(sigmoid(A[-1]))\n",
    "\n",
    "        # When Y is none, simply return the predictions. Else compute the loss\n",
    "        if Y is not None:\n",
    "            loss = -np.sum((1 - Y) * np.log(1 - O[-1]) + Y * np.log(O[-1]))\n",
    "            # normalize loss by batch size\n",
    "            loss = loss.sum() / X.shape[0]\n",
    "        else:\n",
    "            loss = np.nan\n",
    "\n",
    "        if do_backward:\n",
    "            A_grad = [(O[-1] - Y) / X.shape[0]]\n",
    "            self.b_grad = []\n",
    "            self.W_grad = []\n",
    "\n",
    "            for idx in range(len(self.W) - 1):\n",
    "                self.b_grad.append(A_grad[idx].sum(0))\n",
    "                self.W_grad.append(np.dot(A_grad[idx].T, O[-(idx + 2)]))\n",
    "                O_grad = np.dot(A_grad[idx], self.W[-(idx + 1)])\n",
    "                A_grad.append(O_grad * O[-(idx + 2)] * (1 - O[-(idx + 2)]))\n",
    "\n",
    "            self.b_grad.append(A_grad[-1].sum(0))\n",
    "            self.W_grad.append(np.dot(A_grad[-1].T, X))\n",
    "\n",
    "            self.b_grad = self.b_grad[::-1]\n",
    "            self.W_grad = self.W_grad[::-1]\n",
    "        return O[-1], loss\n",
    "\n",
    "    def backward(self, alpha: float = 0.1):\n",
    "        for W, W_grad in zip(self.W, self.W_grad):\n",
    "            W[:] = W[:] - alpha * W_grad\n",
    "\n",
    "        for b, b_grad in zip(self.b, self.b_grad):\n",
    "            b[:] = b[:] - alpha * b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.fmax(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "class Net:\n",
    "    def __init__(self, in_features, num_hidden: Union[List[int], int], dtype=np.float32, activation=\"sigmoid\"):\n",
    "        if isinstance(num_hidden, int):\n",
    "            num_hidden = [num_hidden]\n",
    "\n",
    "        self.activation = activation.lower()\n",
    "\n",
    "        # First layer\n",
    "        self.W = [np.zeros((num_hidden[0], in_features), dtype=dtype)]\n",
    "        self.b = [np.zeros((num_hidden[0],), dtype=dtype)]\n",
    "\n",
    "        # Add other layers\n",
    "        for i in range(1, len(num_hidden)):\n",
    "            self.W.append(np.zeros((num_hidden[i], num_hidden[i - 1]), dtype=dtype))\n",
    "            self.b.append(np.zeros((num_hidden[i], ), dtype=dtype))\n",
    "\n",
    "        # Last layer\n",
    "        self.W.append(np.zeros((1, num_hidden[-1]), dtype=dtype))\n",
    "        self.b.append(np.zeros((1, ), dtype=dtype))\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for W, b in zip(self.W, self.b):\n",
    "            W[:] = np.random.normal(0, 0.5, W.shape)\n",
    "            b[:] = np.random.normal(0, 0.5, b.shape)\n",
    "\n",
    "    def forward(self, X, Y=None, do_backward=False):\n",
    "        # Input to neurons in 1st layer\n",
    "        A = [np.dot(X, self.W[0].T) + self.b[0]]\n",
    "        if self.activation == \"sigmoid\":\n",
    "            O = [sigmoid(A[0])]\n",
    "        elif self.activation == \"relu\":\n",
    "            O = [ReLU(A[0])]\n",
    "        else:\n",
    "            raise ValueError(\"activation not known.\")\n",
    "\n",
    "        for W, b in zip(self.W[1:-1], self.b[1:-1]):\n",
    "            A.append(np.dot(O[-1], W.T) + b)\n",
    "            if self.activation == \"sigmoid\":\n",
    "                O.append(sigmoid(A[-1]))\n",
    "            elif self.activation == \"relu\":\n",
    "                O.append(ReLU(A[-1]))\n",
    "            else:\n",
    "                raise ValueError(\"activation not known.\")\n",
    "            \n",
    "        A.append(np.dot(O[-1], self.W[-1].T) + self.b[-1])\n",
    "        O.append(sigmoid(A[-1]))\n",
    "\n",
    "        # When Y is none, simply return the predictions. Else compute the loss\n",
    "        if Y is not None:\n",
    "            loss = -np.sum((1 - Y) * np.log(1 - O[-1]) + Y * np.log(O[-1]))\n",
    "            # normalize loss by batch size\n",
    "            loss = loss.sum() / X.shape[0]\n",
    "        else:\n",
    "            loss = np.nan\n",
    "\n",
    "        if do_backward:\n",
    "            A_grad = [(O[-1] - Y) / X.shape[0]]\n",
    "            self.b_grad = []\n",
    "            self.W_grad = []\n",
    "\n",
    "            for idx in range(len(self.W) - 1):\n",
    "                self.b_grad.append(A_grad[idx].sum(0))\n",
    "                self.W_grad.append(np.dot(A_grad[idx].T, O[-(idx + 2)]))\n",
    "                O_grad = np.dot(A_grad[idx], self.W[-(idx + 1)])\n",
    "                if self.activation == \"sigmoid\":\n",
    "                    A_grad.append(O_grad * O[-(idx + 2)] * (1 - O[-(idx + 2)]))\n",
    "                elif self.activation == \"relu\":\n",
    "                    A_grad.append(O_grad * np.where(O[-(idx + 2)] == 0, 0, 1))\n",
    "                else:\n",
    "                    raise ValueError(\"activation not known.\")\n",
    "\n",
    "            self.b_grad.append(A_grad[-1].sum(0))\n",
    "            self.W_grad.append(np.dot(A_grad[-1].T, X))\n",
    "\n",
    "            self.b_grad = self.b_grad[::-1]\n",
    "            self.W_grad = self.W_grad[::-1]\n",
    "        return O[-1], loss\n",
    "\n",
    "    def backward(self, alpha: float = 0.1):\n",
    "        for W, W_grad in zip(self.W, self.W_grad):\n",
    "            W[:] = W[:] - alpha * W_grad\n",
    "\n",
    "        for b, b_grad in zip(self.b, self.b_grad):\n",
    "            b[:] = b[:] - alpha * b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after      0 steps \tloss=7.142241e-01 \taccuracy=0.5\n",
      "after   5000 steps \tloss=6.908583e-01 \taccuracy=0.75\n",
      "after  10000 steps \tloss=2.216814e-02 \taccuracy=1.0\n",
      "after  15000 steps \tloss=3.333740e-03 \taccuracy=1.0\n",
      "after  20000 steps \tloss=1.653200e-03 \taccuracy=1.0\n",
      "after  25000 steps \tloss=1.072709e-03 \taccuracy=1.0\n",
      "after  30000 steps \tloss=7.849885e-04 \taccuracy=1.0\n",
      "after  35000 steps \tloss=6.149802e-04 \taccuracy=1.0\n",
      "after  40000 steps \tloss=5.034118e-04 \taccuracy=1.0\n",
      "after  45000 steps \tloss=4.248899e-04 \taccuracy=1.0\n",
      "after  50000 steps \tloss=3.667919e-04 \taccuracy=1.0\n",
      "after  55000 steps \tloss=3.221603e-04 \taccuracy=1.0\n",
      "after  60000 steps \tloss=2.868562e-04 \taccuracy=1.0\n",
      "after  65000 steps \tloss=2.582686e-04 \taccuracy=1.0\n",
      "after  70000 steps \tloss=2.346714e-04 \taccuracy=1.0\n",
      "after  75000 steps \tloss=2.148795e-04 \taccuracy=1.0\n",
      "after  80000 steps \tloss=1.980530e-04 \taccuracy=1.0\n",
      "after  85000 steps \tloss=1.835805e-04 \taccuracy=1.0\n",
      "after  90000 steps \tloss=1.710068e-04 \taccuracy=1.0\n",
      "after  95000 steps \tloss=1.599862e-04 \taccuracy=1.0\n"
     ]
    }
   ],
   "source": [
    "net = Net(3, 2 * [10], np.float64)\n",
    "for i in range(100000):\n",
    "    y_hat, loss = net.forward(X3, Y3, do_backward=True)\n",
    "    net.backward(0.1)\n",
    "    if (i % 5000) == 0:\n",
    "        print(f\"after {i:6d} steps \\tloss={loss:.6e} \\taccuracy={(y_hat.round() == Y3).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after      0 steps \tloss=7.618406e-01 \taccuracy=0.5\n",
      "after   5000 steps \tloss=5.227723e-04 \taccuracy=1.0\n",
      "after  10000 steps \tloss=2.110818e-04 \taccuracy=1.0\n",
      "after  15000 steps \tloss=1.268178e-04 \taccuracy=1.0\n",
      "after  20000 steps \tloss=8.893118e-05 \taccuracy=1.0\n",
      "after  25000 steps \tloss=6.470795e-05 \taccuracy=1.0\n",
      "after  30000 steps \tloss=5.053944e-05 \taccuracy=1.0\n",
      "after  35000 steps \tloss=4.136753e-05 \taccuracy=1.0\n",
      "after  40000 steps \tloss=3.492309e-05 \taccuracy=1.0\n",
      "after  45000 steps \tloss=3.014629e-05 \taccuracy=1.0\n",
      "after  50000 steps \tloss=2.647058e-05 \taccuracy=1.0\n",
      "after  55000 steps \tloss=2.355596e-05 \taccuracy=1.0\n",
      "after  60000 steps \tloss=2.119411e-05 \taccuracy=1.0\n",
      "after  65000 steps \tloss=1.924069e-05 \taccuracy=1.0\n",
      "after  70000 steps \tloss=1.760132e-05 \taccuracy=1.0\n",
      "after  75000 steps \tloss=1.620738e-05 \taccuracy=1.0\n",
      "after  80000 steps \tloss=1.500689e-05 \taccuracy=1.0\n",
      "after  85000 steps \tloss=1.396399e-05 \taccuracy=1.0\n",
      "after  90000 steps \tloss=1.304957e-05 \taccuracy=1.0\n",
      "after  95000 steps \tloss=1.224217e-05 \taccuracy=1.0\n"
     ]
    }
   ],
   "source": [
    "net = Net(3, 2 * [10], np.float64, \"relu\")\n",
    "for i in range(100000):\n",
    "    y_hat, loss = net.forward(X3, Y3, do_backward=True)\n",
    "    net.backward(0.1)\n",
    "    if (i % 5000) == 0:\n",
    "        print(f\"after {i:6d} steps \\tloss={loss:.6e} \\taccuracy={(y_hat.round() == Y3).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after      0 steps \tloss=7.170589e-01 \taccuracy=0.375\n",
      "after   5000 steps \tloss=7.769157e-05 \taccuracy=1.0\n",
      "after  10000 steps \tloss=3.294207e-05 \taccuracy=1.0\n",
      "after  15000 steps \tloss=2.020856e-05 \taccuracy=1.0\n",
      "after  20000 steps \tloss=1.434533e-05 \taccuracy=1.0\n",
      "after  25000 steps \tloss=1.101885e-05 \taccuracy=1.0\n",
      "after  30000 steps \tloss=8.895240e-06 \taccuracy=1.0\n",
      "after  35000 steps \tloss=7.431320e-06 \taccuracy=1.0\n",
      "after  40000 steps \tloss=6.362035e-06 \taccuracy=1.0\n",
      "after  45000 steps \tloss=5.550301e-06 \taccuracy=1.0\n",
      "after  50000 steps \tloss=4.913614e-06 \taccuracy=1.0\n",
      "after  55000 steps \tloss=4.401567e-06 \taccuracy=1.0\n",
      "after  60000 steps \tloss=3.981560e-06 \taccuracy=1.0\n",
      "after  65000 steps \tloss=3.631174e-06 \taccuracy=1.0\n",
      "after  70000 steps \tloss=3.334770e-06 \taccuracy=1.0\n",
      "after  75000 steps \tloss=3.080794e-06 \taccuracy=1.0\n",
      "after  80000 steps \tloss=2.861111e-06 \taccuracy=1.0\n",
      "after  85000 steps \tloss=2.669232e-06 \taccuracy=1.0\n",
      "after  90000 steps \tloss=2.499982e-06 \taccuracy=1.0\n",
      "after  95000 steps \tloss=2.350134e-06 \taccuracy=1.0\n"
     ]
    }
   ],
   "source": [
    "net = Net(3, 5 * [8], np.float64, \"relu\")\n",
    "for i in range(100000):\n",
    "    y_hat, loss = net.forward(X3, Y3, do_backward=True)\n",
    "    net.backward(0.1)\n",
    "    if (i % 5000) == 0:\n",
    "        print(f\"after {i:6d} steps \\tloss={loss:.6e} \\taccuracy={(y_hat.round() == Y3).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after      0 steps \tloss=7.144741e-01 \taccuracy=0.5\n",
      "after   5000 steps \tloss=1.676167e-05 \taccuracy=1.0\n",
      "after  10000 steps \tloss=7.225811e-06 \taccuracy=1.0\n",
      "after  15000 steps \tloss=4.457482e-06 \taccuracy=1.0\n",
      "after  20000 steps \tloss=3.173394e-06 \taccuracy=1.0\n",
      "after  25000 steps \tloss=2.441804e-06 \taccuracy=1.0\n",
      "after  30000 steps \tloss=1.977088e-06 \taccuracy=1.0\n",
      "after  35000 steps \tloss=1.665863e-06 \taccuracy=1.0\n",
      "after  40000 steps \tloss=1.437857e-06 \taccuracy=1.0\n",
      "after  45000 steps \tloss=1.262153e-06 \taccuracy=1.0\n",
      "after  50000 steps \tloss=1.122875e-06 \taccuracy=1.0\n",
      "after  55000 steps \tloss=1.009896e-06 \taccuracy=1.0\n",
      "after  60000 steps \tloss=9.168572e-07 \taccuracy=1.0\n",
      "after  65000 steps \tloss=8.387609e-07 \taccuracy=1.0\n",
      "after  70000 steps \tloss=7.722944e-07 \taccuracy=1.0\n",
      "after  75000 steps \tloss=7.150736e-07 \taccuracy=1.0\n",
      "after  80000 steps \tloss=6.653243e-07 \taccuracy=1.0\n",
      "after  85000 steps \tloss=6.217093e-07 \taccuracy=1.0\n",
      "after  90000 steps \tloss=5.831743e-07 \taccuracy=1.0\n",
      "after  95000 steps \tloss=5.489009e-07 \taccuracy=1.0\n"
     ]
    }
   ],
   "source": [
    "net = Net(3, 7 * [8], np.float64, \"relu\")\n",
    "for i in range(100000):\n",
    "    y_hat, loss = net.forward(X3, Y3, do_backward=True)\n",
    "    net.backward(0.1)\n",
    "    if (i % 5000) == 0:\n",
    "        print(f\"after {i:6d} steps \\tloss={loss:.6e} \\taccuracy={(y_hat.round() == Y3).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after      0 steps \tloss=7.480972e-01 \taccuracy=0.5\n",
      "after   5000 steps \tloss=8.599217e-04 \taccuracy=1.0\n",
      "after  10000 steps \tloss=2.250264e-04 \taccuracy=1.0\n",
      "after  15000 steps \tloss=1.104276e-04 \taccuracy=1.0\n",
      "after  20000 steps \tloss=6.864752e-05 \taccuracy=1.0\n",
      "after  25000 steps \tloss=4.816838e-05 \taccuracy=1.0\n",
      "after  30000 steps \tloss=3.633904e-05 \taccuracy=1.0\n",
      "after  35000 steps \tloss=2.878966e-05 \taccuracy=1.0\n",
      "after  40000 steps \tloss=2.360726e-05 \taccuracy=1.0\n",
      "after  45000 steps \tloss=1.986505e-05 \taccuracy=1.0\n",
      "after  50000 steps \tloss=1.705091e-05 \taccuracy=1.0\n",
      "after  55000 steps \tloss=1.487380e-05 \taccuracy=1.0\n",
      "after  60000 steps \tloss=1.314251e-05 \taccuracy=1.0\n",
      "after  65000 steps \tloss=1.174061e-05 \taccuracy=1.0\n",
      "after  70000 steps \tloss=1.058173e-05 \taccuracy=1.0\n",
      "after  75000 steps \tloss=9.612521e-06 \taccuracy=1.0\n",
      "after  80000 steps \tloss=8.790846e-06 \taccuracy=1.0\n",
      "after  85000 steps \tloss=8.087040e-06 \taccuracy=1.0\n",
      "after  90000 steps \tloss=7.477914e-06 \taccuracy=1.0\n",
      "after  95000 steps \tloss=6.945760e-06 \taccuracy=1.0\n"
     ]
    }
   ],
   "source": [
    "net = Net(3, 10 * [8], np.float64, \"relu\")\n",
    "for i in range(100000):\n",
    "    y_hat, loss = net.forward(X3, Y3, do_backward=True)\n",
    "    net.backward(0.1)\n",
    "    if (i % 5000) == 0:\n",
    "        print(f\"after {i:6d} steps \\tloss={loss:.6e} \\taccuracy={(y_hat.round() == Y3).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after      0 steps \tloss=7.002463e-01 \taccuracy=0.5\n",
      "after   2000 steps \tloss=3.127763e-01 \taccuracy=0.875\n",
      "after   4000 steps \tloss=3.127597e-01 \taccuracy=0.875\n",
      "after   6000 steps \tloss=3.127563e-01 \taccuracy=0.875\n",
      "after   8000 steps \tloss=3.127548e-01 \taccuracy=0.875\n",
      "after  10000 steps \tloss=3.127540e-01 \taccuracy=0.875\n",
      "after  12000 steps \tloss=3.127535e-01 \taccuracy=0.875\n",
      "after  14000 steps \tloss=3.127532e-01 \taccuracy=0.875\n",
      "after  16000 steps \tloss=3.127530e-01 \taccuracy=0.875\n",
      "after  18000 steps \tloss=3.127528e-01 \taccuracy=0.875\n",
      "after  20000 steps \tloss=3.127526e-01 \taccuracy=0.875\n",
      "after  22000 steps \tloss=3.127525e-01 \taccuracy=0.875\n",
      "after  24000 steps \tloss=3.127524e-01 \taccuracy=0.875\n",
      "after  26000 steps \tloss=3.127531e-01 \taccuracy=0.875\n",
      "after  28000 steps \tloss=3.127528e-01 \taccuracy=0.875\n"
     ]
    }
   ],
   "source": [
    "net = Net(3, 12 * [8], np.float64, \"relu\")\n",
    "for i in range(30000):\n",
    "    y_hat, loss = net.forward(X3, Y3, do_backward=True)\n",
    "    net.backward(0.1)\n",
    "    if (i % 2000) == 0:\n",
    "        print(f\"after {i:6d} steps \\tloss={loss:.6e} \\taccuracy={(y_hat.round() == Y3).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after      0 steps \tloss=7.144741e-01 \taccuracy=0.5\n",
      "after   5000 steps \tloss=1.676167e-05 \taccuracy=1.0\n",
      "after  10000 steps \tloss=7.225811e-06 \taccuracy=1.0\n",
      "after  15000 steps \tloss=4.457482e-06 \taccuracy=1.0\n",
      "after  20000 steps \tloss=3.173394e-06 \taccuracy=1.0\n",
      "after  25000 steps \tloss=2.441804e-06 \taccuracy=1.0\n",
      "after  30000 steps \tloss=1.977088e-06 \taccuracy=1.0\n",
      "after  35000 steps \tloss=1.665863e-06 \taccuracy=1.0\n",
      "after  40000 steps \tloss=1.437857e-06 \taccuracy=1.0\n",
      "after  45000 steps \tloss=1.262153e-06 \taccuracy=1.0\n",
      "after  50000 steps \tloss=1.122875e-06 \taccuracy=1.0\n",
      "after  55000 steps \tloss=1.009896e-06 \taccuracy=1.0\n",
      "after  60000 steps \tloss=9.168572e-07 \taccuracy=1.0\n",
      "after  65000 steps \tloss=8.387609e-07 \taccuracy=1.0\n",
      "after  70000 steps \tloss=7.722944e-07 \taccuracy=1.0\n",
      "after  75000 steps \tloss=7.150736e-07 \taccuracy=1.0\n",
      "after  80000 steps \tloss=6.653243e-07 \taccuracy=1.0\n",
      "after  85000 steps \tloss=6.217093e-07 \taccuracy=1.0\n",
      "after  90000 steps \tloss=5.831743e-07 \taccuracy=1.0\n",
      "after  95000 steps \tloss=5.489009e-07 \taccuracy=1.0\n"
     ]
    }
   ],
   "source": [
    "net = Net(3, 7 * [8], np.float64, \"relu\")\n",
    "for i in range(100000):\n",
    "    y_hat, loss = net.forward(X3, Y3, do_backward=True)\n",
    "    net.backward(0.1)\n",
    "    if (i % 5000) == 0:\n",
    "        print(f\"after {i:6d} steps \\tloss={loss:.6e} \\taccuracy={(y_hat.round() == Y3).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after      0 steps \tloss=9.011177e-01 \taccuracy=0.5\n",
      "after   2000 steps \tloss=6.931472e-01 \taccuracy=0.625\n",
      "after   4000 steps \tloss=6.931472e-01 \taccuracy=0.625\n",
      "after   6000 steps \tloss=6.931472e-01 \taccuracy=0.625\n",
      "after   8000 steps \tloss=6.931472e-01 \taccuracy=0.625\n",
      "after  10000 steps \tloss=6.931472e-01 \taccuracy=0.625\n",
      "after  12000 steps \tloss=6.931472e-01 \taccuracy=0.625\n",
      "after  14000 steps \tloss=6.931472e-01 \taccuracy=0.625\n",
      "after  16000 steps \tloss=6.931472e-01 \taccuracy=0.625\n",
      "after  18000 steps \tloss=6.931472e-01 \taccuracy=0.625\n",
      "after  20000 steps \tloss=6.931472e-01 \taccuracy=0.625\n",
      "after  22000 steps \tloss=6.931472e-01 \taccuracy=0.625\n",
      "after  24000 steps \tloss=6.931472e-01 \taccuracy=0.625\n",
      "after  26000 steps \tloss=6.931472e-01 \taccuracy=0.625\n",
      "after  28000 steps \tloss=6.931472e-01 \taccuracy=0.625\n"
     ]
    }
   ],
   "source": [
    "net = Net(3, 12 * [8], np.float64, \"sigmoid\")\n",
    "for i in range(30000):\n",
    "    y_hat, loss = net.forward(X3, Y3, do_backward=True)\n",
    "    net.backward(0.1)\n",
    "    if (i % 2000) == 0:\n",
    "        print(f\"after {i:6d} steps \\tloss={loss:.6e} \\taccuracy={(y_hat.round() == Y3).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00031-032edb86-1c56-46e7-8ea3-3e011d72bf5f",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 186
    },
    "deepnote_cell_type": "markdown",
    "id": "nWuv7Q77-Nut"
   },
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "cell_id": "00032-2eab2b1a-f83b-409c-a066-1642cd2ab2d8",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 192
    },
    "deepnote_cell_type": "code",
    "id": "avuvSoWY-N4Z"
   },
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "class FeedbackAllignmentNet:\n",
    "    def __init__(self, in_features, num_hidden: Union[List[int], int], dtype=np.float32, activation=\"sigmoid\"):\n",
    "        if isinstance(num_hidden, int):\n",
    "            num_hidden = [num_hidden]\n",
    "\n",
    "        self.activation = activation.lower()\n",
    "\n",
    "        # First layer\n",
    "        self.W = [np.zeros((num_hidden[0], in_features), dtype=dtype)]\n",
    "        self.b = [np.zeros((num_hidden[0],), dtype=dtype)]\n",
    "\n",
    "        # Add other layers\n",
    "        for i in range(1, len(num_hidden)):\n",
    "            self.W.append(np.zeros((num_hidden[i], num_hidden[i - 1]), dtype=dtype))\n",
    "            self.b.append(np.zeros((num_hidden[i], ), dtype=dtype))\n",
    "\n",
    "        # Last layer\n",
    "        self.W.append(np.zeros((1, num_hidden[-1]), dtype=dtype))\n",
    "        self.b.append(np.zeros((1, ), dtype=dtype))\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        self.backward_weights = []\n",
    "\n",
    "        for W, b in zip(self.W, self.b):\n",
    "            W[:] = np.random.normal(0, 0.5, W.shape)\n",
    "            b[:] = np.random.normal(0, 0.5, b.shape)\n",
    "            self.backward_weights.append(np.random.normal(0, 0.5, W.T.shape))\n",
    "\n",
    "    def forward(self, X, Y=None, do_backward=False):\n",
    "        # Input to neurons in 1st layer\n",
    "        A = [np.dot(X, self.W[0].T) + self.b[0]]\n",
    "        if self.activation == \"sigmoid\":\n",
    "            O = [sigmoid(A[0])]\n",
    "        elif self.activation == \"relu\":\n",
    "            O = [ReLU(A[0])]\n",
    "        else:\n",
    "            raise ValueError(\"activation not known.\")\n",
    "\n",
    "        for W, b in zip(self.W[1:-1], self.b[1:-1]):\n",
    "            A.append(np.dot(O[-1], W.T) + b)\n",
    "            if self.activation == \"sigmoid\":\n",
    "                O.append(sigmoid(A[-1]))\n",
    "            elif self.activation == \"relu\":\n",
    "                O.append(ReLU(A[-1]))\n",
    "            else:\n",
    "                raise ValueError(\"activation not known.\")\n",
    "            \n",
    "        A.append(np.dot(O[-1], self.W[-1].T) + self.b[-1])\n",
    "        O.append(sigmoid(A[-1]))\n",
    "\n",
    "        # When Y is none, simply return the predictions. Else compute the loss\n",
    "        if Y is not None:\n",
    "            loss = -np.sum((1 - Y) * np.log(1 - O[-1]) + Y * np.log(O[-1]))\n",
    "            # normalize loss by batch size\n",
    "            loss = loss.sum() / X.shape[0]\n",
    "        else:\n",
    "            loss = np.nan\n",
    "\n",
    "        if do_backward:\n",
    "            A_grad = [(O[-1] - Y) / X.shape[0]]\n",
    "            self.b_grad = []\n",
    "            self.W_grad = []\n",
    "\n",
    "            for idx in range(len(self.W) - 1):\n",
    "                self.b_grad.append(A_grad[idx].sum(0))\n",
    "                self.W_grad.append(np.dot(A_grad[idx].T, O[-(idx + 2)]))\n",
    "                O_grad = np.dot(A_grad[idx], self.backward_weights[-(idx + 1)].T)\n",
    "                if self.activation == \"sigmoid\":\n",
    "                    A_grad.append(O_grad * O[-(idx + 2)] * (1 - O[-(idx + 2)]))\n",
    "                elif self.activation == \"relu\":\n",
    "                    A_grad.append(O_grad * np.where(O[-(idx + 2)] == 0, 0, 1))\n",
    "                else:\n",
    "                    raise ValueError(\"activation not known.\")\n",
    "\n",
    "            self.b_grad.append(A_grad[-1].sum(0))\n",
    "            self.W_grad.append(np.dot(A_grad[-1].T, X))\n",
    "\n",
    "            self.b_grad = self.b_grad[::-1]\n",
    "            self.W_grad = self.W_grad[::-1]\n",
    "        return O[-1], loss\n",
    "\n",
    "    def backward(self, alpha: float = 0.1):\n",
    "        for W, W_grad in zip(self.W, self.W_grad):\n",
    "            W[:] = W[:] - alpha * W_grad\n",
    "\n",
    "        for b, b_grad in zip(self.b, self.b_grad):\n",
    "            b[:] = b[:] - alpha * b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after      0 steps \tloss=6.938239e-01 \taccuracy=0.5\n",
      "after   2000 steps \tloss=6.931487e-01 \taccuracy=0.625\n",
      "after   4000 steps \tloss=6.931488e-01 \taccuracy=0.625\n",
      "after   6000 steps \tloss=6.931488e-01 \taccuracy=0.625\n",
      "after   8000 steps \tloss=6.931488e-01 \taccuracy=0.625\n",
      "after  10000 steps \tloss=6.931488e-01 \taccuracy=0.625\n",
      "after  12000 steps \tloss=6.931488e-01 \taccuracy=0.625\n",
      "after  14000 steps \tloss=6.931488e-01 \taccuracy=0.625\n",
      "after  16000 steps \tloss=6.931488e-01 \taccuracy=0.625\n",
      "after  18000 steps \tloss=6.931489e-01 \taccuracy=0.625\n",
      "after  20000 steps \tloss=6.931489e-01 \taccuracy=0.625\n",
      "after  22000 steps \tloss=6.931489e-01 \taccuracy=0.625\n",
      "after  24000 steps \tloss=6.931489e-01 \taccuracy=0.625\n",
      "after  26000 steps \tloss=6.931489e-01 \taccuracy=0.625\n",
      "after  28000 steps \tloss=6.931489e-01 \taccuracy=0.625\n"
     ]
    }
   ],
   "source": [
    "net = FeedbackAllignmentNet(3, 5 * [8], np.float64, \"sigmoid\")\n",
    "for i in range(30000):\n",
    "    y_hat, loss = net.forward(X3, Y3, do_backward=True)\n",
    "    net.backward(0.1)\n",
    "    if (i % 2000) == 0:\n",
    "        print(f\"after {i:6d} steps \\tloss={loss:.6e} \\taccuracy={(y_hat.round() == Y3).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after      0 steps \tloss=1.193274e+00 \taccuracy=0.5\n",
      "after   2000 steps \tloss=6.932188e-01 \taccuracy=0.75\n",
      "after   4000 steps \tloss=6.932293e-01 \taccuracy=0.75\n",
      "after   6000 steps \tloss=6.932445e-01 \taccuracy=0.75\n",
      "after   8000 steps \tloss=6.932651e-01 \taccuracy=0.625\n",
      "after  10000 steps \tloss=6.932888e-01 \taccuracy=0.5\n",
      "after  12000 steps \tloss=6.932956e-01 \taccuracy=0.5\n",
      "after  14000 steps \tloss=6.931913e-01 \taccuracy=0.5\n",
      "after  16000 steps \tloss=6.926430e-01 \taccuracy=0.75\n",
      "after  18000 steps \tloss=6.904084e-01 \taccuracy=0.75\n",
      "after  20000 steps \tloss=6.819219e-01 \taccuracy=0.75\n",
      "after  22000 steps \tloss=6.593111e-01 \taccuracy=0.75\n",
      "after  24000 steps \tloss=5.958851e-01 \taccuracy=0.75\n",
      "after  26000 steps \tloss=4.164928e-01 \taccuracy=1.0\n",
      "after  28000 steps \tloss=2.113350e-01 \taccuracy=1.0\n"
     ]
    }
   ],
   "source": [
    "net = FeedbackAllignmentNet(3, 2 * [8], np.float64, \"sigmoid\")\n",
    "for i in range(30000):\n",
    "    y_hat, loss = net.forward(X3, Y3, do_backward=True)\n",
    "    net.backward(0.1)\n",
    "    if (i % 2000) == 0:\n",
    "        print(f\"after {i:6d} steps \\tloss={loss:.6e} \\taccuracy={(y_hat.round() == Y3).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Assignment1.ipynb",
   "provenance": []
  },
  "deepnote": {},
  "deepnote_app_layout": "article",
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "1f038069-bae5-44e5-92ba-67bdff2c54a6",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
