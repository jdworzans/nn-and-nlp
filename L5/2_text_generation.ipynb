{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faca5110",
   "metadata": {},
   "source": [
    "# Assigment 5\n",
    "\n",
    "**Submission deadlines**:\n",
    "\n",
    "* last lab before 27.06.2022 \n",
    "\n",
    "**Points:** Aim to get 12 out of 15+ possible points\n",
    "\n",
    "All needed data files are on Drive: <https://drive.google.com/drive/folders/1uufpGn46Mwv4oBwajIeOj4rvAK96iaS-?usp=sharing> (or will be soon :) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a158dfd",
   "metadata": {},
   "source": [
    "## Task 2 (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7697542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5bce97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e4de8",
   "metadata": {},
   "source": [
    "**A**. Create text corpora containing texts with similar vocabulary (for instance books from the same genre, or written by the same author). This corpora should have approximately 1M words. You can consider using the following sources: Project Gutenberg (https://www.gutenberg.org/), Wolne Lektury (https://wolnelektury.pl/), parts of BookCorpus, https://github.com/soskek/bookcorpus, but generally feel free. Texts could be in English, Polish or any other language you know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc3d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPORA_FILEPATH = Path(\"data/prus.txt\")\n",
    "\n",
    "if not CORPORA_FILEPATH.exists():\n",
    "    with CORPORA_FILEPATH.open(\"wt\") as f:\n",
    "        for filepath in Path(\"data/prus\").glob(\"*.txt\"):\n",
    "            f.write(filepath.open(\"rt\").read())\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "503063f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061380"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora = CORPORA_FILEPATH.open(\"rt\").read()\n",
    "len(corpora.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b017c",
   "metadata": {},
   "source": [
    "**B**. choose the tokenization procedure. It should have two stages:\n",
    "\n",
    "1. word tokenization (you can use nltk.tokenize.word_tokenize, tokenizer from spaCy, pytorch, keras, ...). Test your tokenizer on your corpora, and look at a set of tokens containing both letters and special characters. If some of them should be in your opinion treated as a sequence of tokens, then modify the tokenization procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "062c09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_TOKENS_FILEPATH = Path(\"data/tokens.pickle\")\n",
    "\n",
    "if BASE_TOKENS_FILEPATH.exists():\n",
    "    with BASE_TOKENS_FILEPATH.open(\"rb\") as f:\n",
    "        tokens = pickle.load(f)\n",
    "else:\n",
    "    tokens = word_tokenize(corpora, \"polish\")\n",
    "    with BASE_TOKENS_FILEPATH.open(\"wb\") as f:\n",
    "        pickle.dump(tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "619c31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = []\n",
    "for token in tokens:\n",
    "    new_tokens.extend(token.replace(\"…\", \"$$…$$\").split(\"$$\"))\n",
    "tokens = new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad6c65",
   "metadata": {},
   "source": [
    "2. sub-word tokenization (you can either use the existing procedure, like wordpiece or sentencepiece, or create something by yourself). Here is a simple idea: take 8K most popular words (W), 1K most popular suffixes (S), and 1K most popular prefixes (P). Words in W are its own tokens. Word x outside W should be tokenized as 'p_ _s' where p is the longest prefix of x in P, and s is the longest prefix of W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cecb7803",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "647dc539",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words = set(next(zip(*word_counts.most_common(8000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ffab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXES_FILEPATH = Path(\"data/fixes.pickle\")\n",
    "\n",
    "if FIXES_FILEPATH.exists():\n",
    "    with FIXES_FILEPATH.open(\"rb\") as f:\n",
    "        suffix_counts, prefix_counts = pickle.load(f)\n",
    "else:\n",
    "    suffix_counts = Counter()\n",
    "    prefix_counts = Counter()\n",
    "    for token in tokens:\n",
    "        for idx in range(len(token) + 1):\n",
    "            prefix_counts.update([token[:idx]])\n",
    "            suffix_counts.update([token[idx:]])\n",
    "    with FIXES_FILEPATH.open(\"wb\") as f:\n",
    "        pickle.dump((suffix_counts, prefix_counts), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb80fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_prefixes = sorted(next(zip(*prefix_counts.most_common(1000))), key=lambda s: -len(s))\n",
    "most_common_suffixes = sorted(next(zip(*suffix_counts.most_common(1000))), key=lambda s: -len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "012dc288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(iterable, n):\n",
    "    for _, el in zip(range(n), iterable):\n",
    "        yield el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "058f39c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_TOKENS_FILEPATH = Path(\"data/tokens_final.pickle\")\n",
    "\n",
    "if FINAL_TOKENS_FILEPATH.exists():\n",
    "    with FINAL_TOKENS_FILEPATH.open(\"rb\") as f:\n",
    "        final_tokens = pickle.load(f)\n",
    "else:\n",
    "    final_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in most_common_words:\n",
    "            final_tokens.append(token)\n",
    "        else:\n",
    "            longest_prefix = \"\"\n",
    "            longest_suffix = \"\"\n",
    "            for prefix in most_common_prefixes:\n",
    "                if token.startswith(prefix):\n",
    "                    longest_prefix = prefix\n",
    "                    break\n",
    "            for suffix in most_common_suffixes:\n",
    "                if token.endswith(suffix):\n",
    "                    longest_suffix = suffix\n",
    "                    break\n",
    "            final_tokens.append(f\"{longest_prefix}__{longest_suffix}\")\n",
    "    with FINAL_TOKENS_FILEPATH.open(\"wb\") as f:\n",
    "        pickle.dump(final_tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bc18f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 10\n",
    "\n",
    "class PrusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokens, sequence_length=SEQUENCE_LENGTH, device=device):\n",
    "        self.tokens = tokens\n",
    "        self.vocab = set(tokens)\n",
    "        self.final_idx_to_word = dict(enumerate(self.vocab))\n",
    "        self.final_word_to_idx = {w: idx for idx, w in self.final_idx_to_word.items()}\n",
    "        self.sequence = torch.tensor([self.final_word_to_idx[token] for token in self.tokens], device=device)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            self.sequence[index:(index + self.sequence_length)],\n",
    "            self.sequence[(index + 1):(index + self.sequence_length + 1)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bd67267",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PrusDataset(final_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9757fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrusModel(nn.Module):\n",
    "    def __init__(self, dataset, device):\n",
    "        super().__init__()\n",
    "        self.n_vocab = len(dataset.vocab)\n",
    "        self.embedding_dim = 100\n",
    "        self.lstm_size = 512\n",
    "        self.num_layers = 3\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, self.n_vocab)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        embed = self.embedding(x)\n",
    "        out, new_state = self.lstm(embed, state)\n",
    "        logits = self.fc(out)\n",
    "        return logits, new_state\n",
    "\n",
    "    def get_init_state(self, sequence_length):\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
    "            torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d9f234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PrusModel(dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "974389dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "max_epochs = 30\n",
    "\n",
    "def train(dataset, model):\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        state_h, state_c = model.get_init_state(SEQUENCE_LENGTH)\n",
    "        \n",
    "        for x, y in tqdm(dataloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()            \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print({ 'epoch': epoch, 'loss': loss.item() })\n",
    "        torch.save(model.state_dict(), f\"prus_model_3x512_{epoch:02d}ep.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fb57193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5097/5097 [34:40<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'loss': 7.259161472320557}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5097/5097 [36:02<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'loss': 5.344584941864014}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5097/5097 [34:48<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'loss': 2.774712324142456}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5097/5097 [34:34<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'loss': 1.6688655614852905}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5097/5097 [34:35<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'loss': 0.8234226107597351}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5097/5097 [34:34<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'loss': 0.5684224963188171}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5097/5097 [34:33<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'loss': 0.42307981848716736}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5097/5097 [34:32<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 7, 'loss': 0.27354881167411804}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5097/5097 [34:33<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 8, 'loss': 0.2113078236579895}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5097/5097 [34:25<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 9, 'loss': 0.15089204907417297}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5097/5097 [34:21<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 10, 'loss': 0.1653604358434677}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 2985/5097 [20:07<14:29,  2.43it/s]"
     ]
    }
   ],
   "source": [
    "train(dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453526b2",
   "metadata": {},
   "source": [
    "\n",
    "This task is about text generation. You have to:\n",
    "\n",
    "\n",
    "**C**. write text generation procedure. The procedure should fulfill the following requirements:\n",
    "\n",
    "1. it should use the RNN language model (trained on sub-word tokens)\n",
    "2. generated tokens should be presented as a text containing words (without extra spaces, or other extra characters, as begin-of-word introduced during tokenization)\n",
    "3. all words in a generated text should belond to the corpora (note that this is not guaranteed by LSTM)\n",
    "4. in generation Top-P sampling should be used (see NN-NLP.6, slide X) \n",
    "5. in generated texts every token 3-gram should be uniq\n",
    "6. *(optionally, +1 point)* all token bigrams in generated texts occur in the corpora"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
