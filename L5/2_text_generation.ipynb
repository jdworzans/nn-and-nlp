{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faca5110",
   "metadata": {},
   "source": [
    "# Assigment 5\n",
    "\n",
    "**Submission deadlines**:\n",
    "\n",
    "* last lab before 27.06.2022 \n",
    "\n",
    "**Points:** Aim to get 12 out of 15+ possible points\n",
    "\n",
    "All needed data files are on Drive: <https://drive.google.com/drive/folders/1uufpGn46Mwv4oBwajIeOj4rvAK96iaS-?usp=sharing> (or will be soon :) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a158dfd",
   "metadata": {},
   "source": [
    "## Task 2 (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453526b2",
   "metadata": {},
   "source": [
    "\n",
    "This task is about text generation. You have to:\n",
    "\n",
    "\n",
    "**C**. write text generation procedure. The procedure should fulfill the following requirements:\n",
    "\n",
    "1. it should use the RNN language model (trained on sub-word tokens)\n",
    "2. generated tokens should be presented as a text containing words (without extra spaces, or other extra characters, as begin-of-word introduced during tokenization)\n",
    "3. all words in a generated text should belond to the corpora (note that this is not guaranteed by LSTM)\n",
    "4. in generation Top-P sampling should be used (see NN-NLP.6, slide X) \n",
    "5. in generated texts every token 3-gram should be uniq\n",
    "6. *(optionally, +1 point)* all token bigrams in generated texts occur in the corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7697542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdworzans/miniconda3/envs/nn-and-nlp/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from utils import PrusDataset, PrusModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd10d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_TOKENS_FILEPATH = Path(\"data/tokens_final.pickle\")\n",
    "POSSIBLE_COMBINATIONS_FILEPATH = Path(\"data/possible_combinations.pickle\")\n",
    "MODEL_FILEPATH = Path(\"model_checkpoints/model-epoch=10-train_loss=6.39.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5bce97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae2bcbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with FINAL_TOKENS_FILEPATH.open(\"rb\") as f:\n",
    "    final_tokens = pickle.load(f)\n",
    "v = torch.load(\"vocab.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd67267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrusModule(\n",
       "  (vocab): Vocab()\n",
       "  (embedding): Embedding(29186, 100)\n",
       "  (lstm): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (fc): Linear(in_features=256, out_features=29186, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = PrusDataset(final_tokens, v)\n",
    "model = PrusModule.load_from_checkpoint(MODEL_FILEPATH)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1368ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "anywhere_idxs = set()\n",
    "for token_idx in range(len(v)):\n",
    "    if not v.lookup_token(token_idx).startswith(\"$\"):\n",
    "        anywhere_idxs.add(token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce77412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if POSSIBLE_COMBINATIONS_FILEPATH.exists():\n",
    "    with POSSIBLE_COMBINATIONS_FILEPATH.open(\"rb\") as f:\n",
    "        possible_idxs_per_idx = pickle.load(f)\n",
    "else:\n",
    "    possible_idxs_per_idx = defaultdict(set)\n",
    "    for idx, token_idx in enumerate(dataset.data):\n",
    "        token = v.lookup_token(token_idx)\n",
    "        if token.endswith(\"$\"):\n",
    "            possible_idxs_per_idx[token_idx.item()].add(dataset.data[idx + 1].item())\n",
    "            if token.startswith(\"$\"):\n",
    "                possible_idxs_per_idx[dataset.data[idx - 1].item(), token_idx.item()].add(dataset.data[idx + 1].item())\n",
    "    with POSSIBLE_COMBINATIONS_FILEPATH.open(\"wb\") as f:\n",
    "        pickle.dump(possible_idxs_per_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "196e1fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate(seed_tokens, next_words=1000, top_p=10):\n",
    "    words = v.lookup_tokens(seed_tokens)\n",
    "    for seed_token in seed_tokens:\n",
    "        y_pred, (state_h, state_c) = model(torch.tensor(seed_token, device=device).reshape(1, 1))\n",
    "\n",
    "    next_token = F.softmax(y_pred.flatten(), -1).argmax()\n",
    "    words.append(v.lookup_token(next_token.item()))\n",
    "    tokens = seed_tokens\n",
    "\n",
    "    unique_trigrams = defaultdict(set)\n",
    "    for t1, t2, t3 in zip(tokens, tokens[1:], tokens[2:]):\n",
    "        unique_trigrams[(t1, t2)].add(t3)\n",
    "\n",
    "    forbidden_tokens = unique_trigrams[tuple(tokens[-2:])]\n",
    "    possible_idxs = torch.tensor(sorted(anywhere_idxs - forbidden_tokens), device=device)\n",
    "\n",
    "    for _ in range(next_words - 1):\n",
    "        y_pred, (state_h, state_c) = model(next_token.reshape(1, 1), (state_h, state_c))\n",
    "\n",
    "        probs = F.softmax(y_pred.flatten()[possible_idxs], -1)\n",
    "        if len(probs) > top_p:\n",
    "            top_p_probs, top_p_idxs = probs.topk(top_p)\n",
    "            selected = torch.multinomial(top_p_probs, 1)\n",
    "            next_token = possible_idxs[top_p_idxs[selected]]\n",
    "        else:\n",
    "            selected = torch.multinomial(probs, 1)\n",
    "            next_token = possible_idxs[selected]\n",
    "\n",
    "        next_word = v.lookup_token(next_token)\n",
    "        words.append(next_word)\n",
    "        unique_trigrams[tuple(tokens[-2:])].add(next_token.item())\n",
    "        tokens.append(next_token.item())\n",
    "\n",
    "        forbidden_tokens = unique_trigrams[tuple(tokens[-2:])]\n",
    "        if next_word.endswith(\"$\"):\n",
    "            if words[-2].endswith(\"$\"):\n",
    "                possible_idxs = torch.tensor(\n",
    "                    sorted(possible_idxs_per_idx[tokens[-2], next_token.item()] - forbidden_tokens),\n",
    "                    device=device,\n",
    "                )\n",
    "            else:\n",
    "                possible_idxs = torch.tensor(\n",
    "                    sorted(possible_idxs_per_idx[next_token.item()] - forbidden_tokens),\n",
    "                    device=device,\n",
    "                )\n",
    "        else:\n",
    "            possible_idxs = torch.tensor(sorted(anywhere_idxs - forbidden_tokens), device=device)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76990fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_text(tokens):\n",
    "    text = \"\"\n",
    "    for word in v.lookup_tokens(tokens):\n",
    "        if not word.startswith(\"$\") and not word in {\".\", \",\", \"?\", \"!\"}:\n",
    "            text += \" \"\n",
    "        text += word.strip(\"$\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f1e1a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtokens(token):\n",
    "    rest = token\n",
    "    subtokens = []\n",
    "    for idx in range(len(rest), 0, -1):\n",
    "        possible_token = f\"{rest[:idx]}$\"\n",
    "        if possible_token in v:\n",
    "            subtokens.append(possible_token)\n",
    "            # print(possible_token)\n",
    "            rest = rest[idx:]\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(f\"Unable to tokenize '{token}'\")\n",
    "\n",
    "    for idx in range(len(rest), 0, -1):\n",
    "        possible_token = f\"${token[-idx:]}\"\n",
    "        if possible_token in v:\n",
    "            subtokens.append(possible_token)\n",
    "            # print(possible_token)\n",
    "            rest = rest[:-idx]\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(f\"Unable to tokenize '{token}'\")\n",
    "    if rest:\n",
    "        possible_token = f\"${rest}$\"\n",
    "        if possible_token in v:\n",
    "            subtokens.append(possible_token)\n",
    "        else:\n",
    "            raise ValueError(f\"Unable to tokenize '{token}'\")\n",
    "    return subtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4b682e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(seed_text):\n",
    "    tokens = []\n",
    "    for base_token in word_tokenize(seed_text.lower()):\n",
    "        for preprocessed_token in filter(lambda s: s, base_token.replace(\"…\", \"$$…$$\").split(\"$$\")):\n",
    "            if preprocessed_token in v:\n",
    "                tokens.append(preprocessed_token)\n",
    "            else:\n",
    "                subtokens = get_subtokens(preprocessed_token)\n",
    "                tokens.extend(subtokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4509f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(seed_text, next_words=1000, top_p=10):\n",
    "    tokens = tokenize(seed_text)\n",
    "    token_idxs = v.lookup_indices(tokens)\n",
    "    all_tokens = _generate(token_idxs, next_words, top_p)\n",
    "    return tokens_to_text(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e58bd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' wokulski powiedział do łęckiej że, nie wiem, że nie nie, a to, nie, co to, że to pan nie wiem. — ja to nie to, co, a co to ja pan, że już ja, to to nie wiem — rzekł, że w tej chwili, że, że ja ja nie wiem!  …  — co nie ma, a ja to pan!  — a to ja, co nie to jest nie ma i nie było nie nie ma!   — spytał, to ja to jest, że na co nie nie nie było się do mnie, co ja, a już ja ja jest, to mi, a nie to pan, nie nie wiem ... — nie ma nie wiem? ... …  nie to — a nie ma mi, że z tej mnie nie wiem  — odparł! i to nie było to, jak to pan jest nie nie mnie na niego, a on nie ma. — to to to, a pan, to pan? ... — odparł … — rzekł — odparł?  … ... — to nie jest, co się nie wiem do niej. …  i już nie, i ja nie nie pani — mówił …  [ przypis redakcyjny. — co to mi!  [ sobie i w chwili, co już nie ma : — to ja jest to, to jest na tej chwili? ... ”! ... ”, nie jest na niego i i na mnie nie nie to nie ma w tym, nie wiadomo i nie jest z mnie. [ przypis ] ] ], który w tej nim na mnie w tej razie jest w tej tej chwili było w chwili. — a co …  … …  ja to to mi mi, to na mnie pan, co na mnie, ja nie ma to? …, że jest to na to nie nie jest i w tym chwili nie było, że pan nie ma się w tej domu — mówił — a ja, ja to, na mnie z tym, że mi pan nie było z tym i i ja to w tym dnia w tej tym, co z to było, a w którym nie wiem …  pan jest, nie ma na mnie!  pan, ale nie wiem i z którym, że mnie, nie było w nim. a nie wiem z mnie, że on, co …, a nawet w chwili nie nie już nie nie wiadomo do mnie — mówił, a i nie nie mógł, nie to! — odparł mi! — rzekł …  w mnie nie jest do siebie, że się w którym jest, i to jest z tej chwili. i w którym na mnie na mnie to, w mnie, w nim, a z tej tej razie, że pani, nie w tej pory nie, nie ja, że tu w którym było w mnie z tej razie nie wiem się, że i to już, że za mnie, a ty nie wiem nie nie będzie — rzekł?  i nie ma mnie, i nie wiem o tej, że był, a gdy nie jest nie wiadomo. — nie nie ja pan nie to … — a już nie wiem : — no, że tak nie ma go z nim, że … ... [ przypis przypis redakcyjny, i w tej niej nie ma dnia na mnie. — i ja pan to jest mi, co jest nie jest w tym mną?  — nie jest pan, a tak mi się nie nie pan. — cóż to, panie, co mi …  ” — rzekł! — spytał mi, jak ja, w końcu, że co ja nie jest to jest do mną, że jeszcze nie było! — nie wiem na siebie, co w chwili to jest to nie będzie. — że już pan — a i w mnie to jest pan — rzekł mi. — no. — ale ja, na tej mnie, to nie, jak nie jest mi! ... — mówił. — czy to, ja mi nie wiem w nim i na co, że ci pan było nie ma sobie. — już ja to …  już, co mnie, jak jest, a na tej siebie! — a pan nie wiadomo, to, o tym, panie! , że — a w tej warszawie, to w mnie w nim to nie mogę nie nie tak, a teraz, nie mnie, panie?  [ się z mnie i na tej tym. — w którym w tej nich w tym tym nie nie mam, co on! — zawołał …   … na mnie?   w nim nie było. a to nie pan jest mnie z nim na tym, a że w mnie pan nie jest już nie było : — a on to nie wiadomo  … — spytał — rzekł. — bo to to na tej dni nie jest pani! …  na to, ale w to jest mnie do niej, że ten jest nie było i w nim z mnie z pokoju i i nie wiadomo na niej na niej. „ nie jest mnie! i co nie wiem ). — więc nie jest za mnie i to to!  i to pan …  o to nie ja jest nie'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"Wokulski powiedział do Łęckiej\", top_p=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nn-and-nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c9edea7d3c7f8afb1e3a5085762e5250f6cb461b15232d4b256ad3000efebc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
