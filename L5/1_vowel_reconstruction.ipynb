{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faca5110",
   "metadata": {},
   "source": [
    "# Assigment 5\n",
    "\n",
    "**Submission deadlines**:\n",
    "\n",
    "* last lab before 27.06.2022 \n",
    "\n",
    "**Points:** Aim to get 12 out of 15+ possible points\n",
    "\n",
    "All needed data files are on Drive: <https://drive.google.com/drive/folders/1uufpGn46Mwv4oBwajIeOj4rvAK96iaS-?usp=sharing> (or will be soon :) )\n",
    "\n",
    "## Task 1 (5 points)\n",
    "\n",
    "Consider the vowel reconstruction task -- i.e. inserting missing vowels (aeuioy) to obtain proper English text. For instance for the input sentence:\n",
    "\n",
    "<pre>\n",
    "h m gd smbd hs stln ll m vwls\n",
    "</pre>\n",
    "\n",
    "the best result is\n",
    "\n",
    "<pre>\n",
    "oh my god somebody has stolen all my vowels\n",
    "</pre>\n",
    "\n",
    "In this task both dev and test data come from the two books about Winnie-the-Pooh. You have to train two RNN Language Models on *pooh-train.txt*. For the first model use the code below, for the second choose different hyperparameters (different dropout, smaller number of units or layers, or just do any modification you want). \n",
    "\n",
    "The code below is based on\n",
    "https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d50e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "SEQUENCE_LENGTH = 15\n",
    "\n",
    "class PoohDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequence_length, device):\n",
    "        txt = open('data/pooh_train.txt').read()\n",
    "        \n",
    "        self.words = txt.lower().split() # The text is already tokenized\n",
    "        \n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+self.sequence_length], device=self.device),\n",
    "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1], device=self.device)\n",
    "        )\n",
    "        \n",
    "pooh_dataset = PoohDataset(SEQUENCE_LENGTH, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b22a87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (embedding): Embedding(2548, 100)\n",
       "  (lstm): LSTM(100, 512, num_layers=2, dropout=0.2)\n",
       "  (fc): Linear(in_features=512, out_features=2548, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, dataset, device):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm_size = 512\n",
    "        self.embedding_dim = 100\n",
    "        self.num_layers = 2\n",
    "        self.device = device\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))\n",
    "        \n",
    "model = LSTMModel(pooh_dataset, device) \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "074d5f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 512\n",
    "max_epochs = 30\n",
    "\n",
    "def train(dataset, model):\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        state_h, state_c = model.init_state(SEQUENCE_LENGTH)\n",
    "        \n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()            \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c08be512",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"pooh_2x512_30ep.model\"\n",
    "try:\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "except FileNotFoundError:\n",
    "    train(pooh_dataset, model)\n",
    "    torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38520dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the morning pooh sound 's very happy thing -- tiggers could n't get down , but somebody you bounced to balloons too . they could have very flying , but nobody seemed . there is another good song . some can find which some come . which does n't shout no any more\n",
      "in the morning piglet hearthrug . because having love him , piglet slipped a pooh behind , some help . `` oh , christopher you , tigger 's perhaps '' began piglet , so that anyhow it was n't . `` come along , '' said tigger . chapter became very thoughtful , because\n",
      "in the morning christopher robin and out of the hundred river which came up behind a left , and now down to make glad cage what happened to christopher robin about the blue door . he looked at his own ceiling which he had left without this in . `` ha , '' said pooh\n",
      "in the morning rabbit christopher robin would do it coming and strong and roo ran before ( or ready of string when we are quite sure we took christopher robin 's helping matter , and before they all were all right together . they crouched seemed very well , and now many ears days\n",
      "in the morning owl 's name and i saw everything . there are itself . owl looked at the sky and ringing down together ; and rabbit looked too ; and here . `` here we are , '' said piglet ten minutes carelessly , `` we shall have done such a blusterous day\n",
      "in the morning tigger told him , and at last it slowly too coming down about .... and anybody cage hums , there and pooh and piglet were all by each side of it , listening it just what it was . there was a loud splash , which pooh does now the only\n",
      "in the morning eeyore and no complain with tigger . if they were just going to keep the happy before owl had had a mysterious , and things seemed trying to be too ; for so when his grandfather got to the bridge and blue getting all through the wood as quickly as they\n"
     ]
    }
   ],
   "source": [
    "def predict(dataset, model, text, next_words=15):\n",
    "    model.eval()\n",
    "\n",
    "    words = text.split()\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
    "        x = x.to(device)\n",
    "        \n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# DEMO\n",
    "speakers = ['pooh', 'piglet', 'christopher robin', 'rabbit', 'owl', 'tigger', 'eeyore']\n",
    "for s in speakers:\n",
    "    prompt = 'in the morning ' + s \n",
    "    for i in range(1):\n",
    "        print(predict(pooh_dataset, model, prompt, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea1eb733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "863\n"
     ]
    }
   ],
   "source": [
    "# You can use the code if you want\n",
    "\n",
    "from collections import defaultdict as dd\n",
    "\n",
    "vowels = set(\"aoiuye'\")\n",
    "def devowelize(s):\n",
    "    rv = ''.join(a for a in s if a not in vowels)\n",
    "    if rv:\n",
    "        return rv\n",
    "    return '_' # Symbol for words without consonants\n",
    "\n",
    "pooh_words = set(open('data/pooh_words.txt').read().split())\n",
    "representation = dd(set)\n",
    "\n",
    "for w in pooh_words:\n",
    "    r = devowelize(w)\n",
    "    representation[r].add(w)\n",
    "    \n",
    "hard_words = set()\n",
    "for r, ws in representation.items():\n",
    "    if len(ws) > 1:\n",
    "        hard_words.update(ws)\n",
    "        \n",
    "print (len(hard_words))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a552ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8c01172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if you happen to have read another book about christopher robin , you may remember that he once had a swan and then you 'll take together for -- but what happened that nobody says everything down as this ? '' `` well , '' said\n"
     ]
    }
   ],
   "source": [
    "text = \"if you happen to have read another book about christopher robin , you may remember that he once had a swan\"\n",
    "words = text.split()\n",
    "state_h, state_c = model.init_state(len(words))\n",
    "x = torch.tensor([[pooh_dataset.word_to_index[w] for w in words]], device=device)\n",
    "next_words = 25\n",
    "for i in range(next_words):\n",
    "    y_pred, (new_state_h, new_state_c) = model(x, (state_h, state_c))\n",
    "    y_pred = y_pred[:, -1:].contiguous()\n",
    "    state_h, state_c = new_state_h[:, -1:, ...].contiguous(), new_state_c[:, -1:, ...].contiguous()\n",
    "    x = torch.multinomial(F.softmax(y_pred, dim=-1).flatten(), 1).reshape(1, -1)\n",
    "    words.append(pooh_dataset.index_to_word[x.item()])\n",
    "print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0015cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, model, text, next_words=15):\n",
    "    model.eval()\n",
    "\n",
    "    words = text.split()\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
    "        x = x.to(device)\n",
    "\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425caf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"pooh\"\n",
    "words = text.split()\n",
    "\n",
    "model.eval()\n",
    "corrected = []\n",
    "state_h, state_c = model.init_state(1)\n",
    "x = torch.tensor([[pooh_dataset.word_to_index[start]]], device=device)\n",
    "for word in words:\n",
    "    possible_idxs = torch.tensor([pooh_dataset.word_to_index[k] for k in representation[word] if k in pooh_dataset.word_to_index])\n",
    "    y_pred, (new_state_h, new_state_c) = model(x, (state_h, state_c))\n",
    "    y_pred = y_pred[:, -1:].contiguous()\n",
    "    state_h, state_c = new_state_h[:, -1:, ...].contiguous(), new_state_c[:, -1:, ...].contiguous()\n",
    "    selected = torch.multinomial(F.softmax(y_pred.flatten()[possible_idxs] / T), 1)\n",
    "    x = possible_idxs[selected].reshape(1, -1)\n",
    "    corrected.append(pooh_dataset.index_to_word[x.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6340fc62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5aa49233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def _reconstruct(words: List[str], model, start: str, T: float):\n",
    "    model.eval()\n",
    "    corrected = []\n",
    "    state_h, state_c = model.init_state(1)\n",
    "    x = torch.tensor([[pooh_dataset.word_to_index[start]]], device=device)\n",
    "    plog = 0\n",
    "    for word in words:\n",
    "        possible_idxs = torch.tensor([pooh_dataset.word_to_index[k] for k in representation[word] if k in pooh_dataset.word_to_index], device=device)\n",
    "        y_pred, (new_state_h, new_state_c) = model(x, (state_h, state_c))\n",
    "        y_pred = y_pred[:, -1:].contiguous()\n",
    "        state_h, state_c = new_state_h[:, -1:, ...].contiguous(), new_state_c[:, -1:, ...].contiguous()\n",
    "        if possible_idxs.numel():\n",
    "            preds = F.softmax(y_pred.flatten()[possible_idxs] / T, -1)\n",
    "        else:\n",
    "            preds = F.softmax(y_pred.flatten() / T, -1)\n",
    "\n",
    "        selected = torch.multinomial(preds, 1)\n",
    "\n",
    "        if possible_idxs.numel():\n",
    "            x = possible_idxs[selected].reshape(1, -1)\n",
    "        else:\n",
    "            x = selected.reshape(1, -1)\n",
    "\n",
    "        corrected.append(pooh_dataset.index_to_word[x.item()])\n",
    "        plog += torch.log(preds[selected]).item()\n",
    "    return \" \".join([start] + corrected), plog\n",
    "\n",
    "def reconstruct(text: List[str], model, T: float, n_iter: int = 10):\n",
    "    if not text:\n",
    "        return []\n",
    "    model.eval()\n",
    "    max_plog = float(\"-inf\")\n",
    "    for start in representation[text[0]]:\n",
    "        for _ in range(n_iter):\n",
    "            correction, plog = _reconstruct(text[1:], model, start, T)\n",
    "            if plog > max_plog:\n",
    "                max_plog = plog\n",
    "                best_correction = correction\n",
    "    return best_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "76ffa7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pooh_test.txt\", \"rt\") as f:\n",
    "    test_tokens = f.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "571af0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = list(map(devowelize, test_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ee361db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = reconstruct(test_input, model, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273881ae",
   "metadata": {},
   "source": [
    "You can assume that only words from pooh_words.txt can occur in the reconstructed text. For decoding you have two options (choose one, or implement both ang get **+1** bonus point)\n",
    "\n",
    "1. Sample reconstructed text several times (with quite a low temperature), choose the most likely result.\n",
    "2. Perform beam search.\n",
    "\n",
    "Of course in the sampling procedure you should consider only words matching the given consonants.\n",
    "\n",
    "Report accuracy of your methods (for both language models). The accuracy should be computed by the following function, it should be *greater than 0.25*.\n",
    "\n",
    "\n",
    "```python\n",
    "def accuracy(original_sequence, reconstructed_sequence):\n",
    "    sa = original_sequence\n",
    "    sb = reconstructed_sequence\n",
    "    score = len([1 for (a,b) in zip(sa, sb) if a == b])\n",
    "    return score / len(original_sequence)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "86a585a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(original_sequence, reconstructed_sequence):\n",
    "    sa = original_sequence\n",
    "    sb = reconstructed_sequence\n",
    "    score = len([1 for (a,b) in zip(sa, sb) if a == b])\n",
    "    return score / len(original_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "be416f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7969128715397372"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(test_tokens, reconstructed.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a158dfd",
   "metadata": {},
   "source": [
    "## Task 2 (6 points)\n",
    "\n",
    "This task is about text generation. You have to:\n",
    "\n",
    "**A**. Create text corpora containing texts with similar vocabulary (for instance books from the same genre, or written by the same author). This corpora should have approximately 1M words. You can consider using the following sources: Project Gutenberg (https://www.gutenberg.org/), Wolne Lektury (https://wolnelektury.pl/), parts of BookCorpus, https://github.com/soskek/bookcorpus, but generally feel free. Texts could be in English, Polish or any other language you know.\n",
    "\n",
    "**B**. choose the tokenization procedure. It should have two stages:\n",
    "\n",
    "1. word tokenization (you can use nltk.tokenize.word_tokenize, tokenizer from spaCy, pytorch, keras, ...). Test your tokenizer on your corpora, and look at a set of tokens containing both letters and special characters. If some of them should be in your opinion treated as a sequence of tokens, then modify the tokenization procedure\n",
    "\n",
    "2. sub-word tokenization (you can either use the existing procedure, like wordpiece or sentencepiece, or create something by yourself). Here is a simple idea: take 8K most popular words (W), 1K most popular suffixes (S), and 1K most popular prefixes (P). Words in W are its own tokens. Word x outside W should be tokenized as 'p_ _s' where p is the longest prefix of x in P, and s is the longest prefix of W\n",
    "\n",
    "**C**. write text generation procedure. The procedure should fulfill the following requirements:\n",
    "\n",
    "1. it should use the RNN language model (trained on sub-word tokens)\n",
    "2. generated tokens should be presented as a text containing words (without extra spaces, or other extra characters, as begin-of-word introduced during tokenization)\n",
    "3. all words in a generated text should belond to the corpora (note that this is not guaranteed by LSTM)\n",
    "4. in generation Top-P sampling should be used (see NN-NLP.6, slide X) \n",
    "5. in generated texts every token 3-gram should be uniq\n",
    "6. *(optionally, +1 point)* all token bigrams in generated texts occur in the corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e84582",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "In this task you have to create a network which looks at characters of the word and tries to guess whether the word is a noun, a verb, an adjective, and so on. To be more precise: the input is a word (without context), the output is a POS-tag (Part-of-Speech). Since some words are unambiguous, and we have no context, our network is supposed to return the set of possible tags.\n",
    "\n",
    "The data is taken from Universal Dependencies English corpus, and of course it contains errors, especially because not all possible tags occured in the data.\n",
    "\n",
    "Train a network (4p) or two networks (+2p) solving this task. Both networks should look at character n-grams occuring in the word. There are two options:\n",
    "\n",
    "* **Fixed size:** for instance take 2,3, and 4-character suffixes of the word, use them as  features (whith 1-hot encoding). You can also combine prefix and suffix features. Simple, useful trick: when looking at suffixes, add some '_' characters at the beginning of the word to guarantee that shorter words have suffixes of a desired length.\n",
    "\n",
    "* **Variable size:** take for instance 4-grams (or 4 grams and 3-grams), use Deep Averaging Network. Simple trick: add extra character at the beginning and at the end of the word, to add the information, that ngram occurs at special position ('ed' at the end has slightly different meaning that 'ed' in the middle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac85fcb",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "Apply seq2seq model (you can modify the code from this tutorial: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) to compute grapheme to phoneme conversion for English. Train the model on dev_cmu_dict.txt and test it on test_cmu_dict.txt. Report accuracy of your solution using two metrics:\n",
    "* exact match (how many words are perfectly converted to phonemes)\n",
    "* exact match without stress (how many words are perfectly converted to phonemes when we remove the information about stress)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da193ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d5e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c538fb76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4feefe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
