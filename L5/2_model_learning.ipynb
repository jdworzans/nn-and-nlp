{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faca5110",
   "metadata": {},
   "source": [
    "# Assigment 5\n",
    "\n",
    "**Submission deadlines**:\n",
    "\n",
    "* last lab before 27.06.2022 \n",
    "\n",
    "**Points:** Aim to get 12 out of 15+ possible points\n",
    "\n",
    "All needed data files are on Drive: <https://drive.google.com/drive/folders/1uufpGn46Mwv4oBwajIeOj4rvAK96iaS-?usp=sharing> (or will be soon :) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a158dfd",
   "metadata": {},
   "source": [
    "## Task 2 (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7697542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import vocab\n",
    "from tqdm import tqdm\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from utils import PrusModel, PrusDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5bce97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e4de8",
   "metadata": {},
   "source": [
    "**A**. Create text corpora containing texts with similar vocabulary (for instance books from the same genre, or written by the same author). This corpora should have approximately 1M words. You can consider using the following sources: Project Gutenberg (https://www.gutenberg.org/), Wolne Lektury (https://wolnelektury.pl/), parts of BookCorpus, https://github.com/soskek/bookcorpus, but generally feel free. Texts could be in English, Polish or any other language you know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc3d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPORA_FILEPATH = Path(\"data/prus.txt\")\n",
    "\n",
    "if not CORPORA_FILEPATH.exists():\n",
    "    with CORPORA_FILEPATH.open(\"wt\") as f:\n",
    "        for filepath in Path(\"data/prus\").glob(\"*.txt\"):\n",
    "            f.write(filepath.open(\"rt\").read())\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "503063f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061380"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora = CORPORA_FILEPATH.open(\"rt\").read()\n",
    "len(corpora.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b017c",
   "metadata": {},
   "source": [
    "**B**. choose the tokenization procedure. It should have two stages:\n",
    "\n",
    "1. word tokenization (you can use nltk.tokenize.word_tokenize, tokenizer from spaCy, pytorch, keras, ...). Test your tokenizer on your corpora, and look at a set of tokens containing both letters and special characters. If some of them should be in your opinion treated as a sequence of tokens, then modify the tokenization procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "062c09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_TOKENS_FILEPATH = Path(\"data/tokens.pickle\")\n",
    "\n",
    "if BASE_TOKENS_FILEPATH.exists():\n",
    "    with BASE_TOKENS_FILEPATH.open(\"rb\") as f:\n",
    "        tokens = pickle.load(f)\n",
    "else:\n",
    "    tokens = word_tokenize(corpora.lower(), \"polish\")\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        new_tokens.extend(token.replace(\"…\", \"$$…$$\").split(\"$$\"))\n",
    "    tokens = new_tokens\n",
    "    with BASE_TOKENS_FILEPATH.open(\"wb\") as f:\n",
    "        pickle.dump(tokens, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad6c65",
   "metadata": {},
   "source": [
    "2. sub-word tokenization (you can either use the existing procedure, like wordpiece or sentencepiece, or create something by yourself). Here is a simple idea: take 8K most popular words (W), 1K most popular suffixes (S), and 1K most popular prefixes (P). Words in W are its own tokens. Word x outside W should be tokenized as 'p_ _s' where p is the longest prefix of x in P, and s is the longest prefix of W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cecb7803",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "647dc539",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words = set(next(zip(*word_counts.most_common(8000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2ffab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXES_FILEPATH = Path(\"data/fixes.pickle\")\n",
    "\n",
    "if FIXES_FILEPATH.exists():\n",
    "    with FIXES_FILEPATH.open(\"rb\") as f:\n",
    "        suffix_counts, prefix_counts = pickle.load(f)\n",
    "else:\n",
    "    suffix_counts = Counter()\n",
    "    prefix_counts = Counter()\n",
    "    for token in tokens:\n",
    "        for idx in range(len(token) + 1):\n",
    "            prefix_counts.update([token[:idx]])\n",
    "            suffix_counts.update([token[idx:]])\n",
    "    with FIXES_FILEPATH.open(\"wb\") as f:\n",
    "        pickle.dump((suffix_counts, prefix_counts), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb80fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_prefixes = sorted(next(zip(*prefix_counts.most_common(1000))), key=lambda s: -len(s))\n",
    "most_common_suffixes = sorted(next(zip(*suffix_counts.most_common(1000))), key=lambda s: -len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "012dc288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(iterable, n):\n",
    "    for _, el in zip(range(n), iterable):\n",
    "        yield el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "058f39c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_TOKENS_FILEPATH = Path(\"data/tokens_final.pickle\")\n",
    "\n",
    "if FINAL_TOKENS_FILEPATH.exists():\n",
    "    with FINAL_TOKENS_FILEPATH.open(\"rb\") as f:\n",
    "        final_tokens = pickle.load(f)\n",
    "    v = torch.load(\"vocab.pth\")\n",
    "else:\n",
    "    final_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in most_common_words:\n",
    "            final_tokens.append(token)\n",
    "        else:\n",
    "            rest = token\n",
    "            subtokens = []\n",
    "            longest_prefix = \"\"\n",
    "            longest_suffix = \"\"\n",
    "            for prefix in most_common_prefixes:\n",
    "                if token.startswith(prefix):\n",
    "                    longest_prefix = prefix\n",
    "                    break\n",
    "            if longest_prefix:\n",
    "                rest = rest[len(longest_prefix):]\n",
    "                subtokens.append(longest_prefix)\n",
    "\n",
    "            for suffix in most_common_suffixes:\n",
    "                if rest.endswith(suffix):\n",
    "                    longest_suffix = suffix\n",
    "                    break\n",
    "\n",
    "            if longest_suffix:\n",
    "                rest = rest[:-len(longest_suffix)]\n",
    "            if rest:\n",
    "                subtokens.append(rest)\n",
    "            if longest_suffix:\n",
    "                subtokens.append(longest_suffix)\n",
    "\n",
    "            if len(subtokens) == 3:\n",
    "                subtokens[0] = subtokens[0] + \"$\"\n",
    "                subtokens[1] = \"$\" + subtokens[1] + \"$\"\n",
    "                subtokens[2] = \"$\" + subtokens[2]\n",
    "\n",
    "            elif len(subtokens) == 2:\n",
    "                subtokens[0] = subtokens[0] + \"$\"\n",
    "                subtokens[1] = \"$\" + subtokens[1]\n",
    "            final_tokens.extend(subtokens)\n",
    "    with FINAL_TOKENS_FILEPATH.open(\"wb\") as f:\n",
    "        pickle.dump(final_tokens, f)\n",
    "    v = vocab(Counter(final_tokens))\n",
    "    v.append_token(\"<unknown>\")\n",
    "    v.set_default_index(-1)\n",
    "    torch.save(v, \"vocab.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
